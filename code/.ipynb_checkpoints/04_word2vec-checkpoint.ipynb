{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Bag-of-Words\" data-toc-modified-id=\"Bag-of-Words-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bag of Words</a></span></li><li><span><a href=\"#Train-Words2Vec\" data-toc-modified-id=\"Train-Words2Vec-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train Words2Vec</a></span></li><li><span><a href=\"#Importing-the-tweets\" data-toc-modified-id=\"Importing-the-tweets-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Importing the tweets</a></span></li><li><span><a href=\"#Creating-the-Urgent-vs.-non-Urgent-Vectors\" data-toc-modified-id=\"Creating-the-Urgent-vs.-non-Urgent-Vectors-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Creating the Urgent vs. non-Urgent Vectors</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **words were created using a logistic regression on an open source database used for NLP that contains messages during disasters and their categorizations, one of those being if the message was a direct call for help, what we will classify as a message being more \"urgent\"**\n",
    "    - https://appen.com/datasets/combined-disaster-response-data/\n",
    "    \n",
    "    \n",
    "- **another way to classify is a message is more urgent is using FEMA's Public Assistance Program and Policy Guide**\n",
    "    - in this guide, public assistance work is either emergency work or permanent work. Important terms from each designation were added to our bag of words \n",
    "    - **Emergency**: \n",
    "        - Emergency Protective Measures\n",
    "        - Debris Removal \n",
    "    - **Permanent**:\n",
    "        - Roads and Bridges\n",
    "        - Water Control Facilities\n",
    "        - Buildings and Equipment\n",
    "        - Utilities \n",
    "        - Parks, Recreational, other \n",
    "        \n",
    "    - https://www.fema.gov/media-library-data/1525468328389-4a038bbef9081cd7dfe7538e7751aa9c/PAPPG_3.1_508_FINAL_5-4-2018.pdf\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of urgent words\n",
    "urgent = ['help',\n",
    "          'campfire',\n",
    "          'tonight',\n",
    "          'today',\n",
    "          'fire',\n",
    "          'need',\n",
    "          'aid',\n",
    "          'removal',\n",
    "          'burn',\n",
    "          'tree',\n",
    "          'evacuation',\n",
    "          'smoke',\n",
    "          'unsafe',\n",
    "          'search',\n",
    "          'lost',\n",
    "          'victim',\n",
    "          'medical',\n",
    "          'med',\n",
    "          'urgent',\n",
    "          'home',\n",
    "          'haze',\n",
    "          'serious',\n",
    "          'fires',\n",
    "          'reporting',\n",
    "          'smoking',\n",
    "          'come',\n",
    "          'emergency',\n",
    "          'wildfire',\n",
    "          'Portland',\n",
    "          'send',\n",
    "          'park',\n",
    "          'valley',\n",
    "          'investigation',\n",
    "          'structure'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of non-urgent words \n",
    "non_urgent = ['job',\n",
    "              'news',\n",
    "              'government',\n",
    "              'country',\n",
    "              'materials',\n",
    "              'work',\n",
    "              'price',\n",
    "              'utilities',\n",
    "              'facility',\n",
    "              'erosion',\n",
    "              'irrigation',\n",
    "              'restoration',\n",
    "              'shoulder',\n",
    "              'mud',\n",
    "              'silt',\n",
    "              'ditch',\n",
    "              'slip',\n",
    "              'pray',\n",
    "              'hope',\n",
    "              'thanks',\n",
    "              'thankful'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Words2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model is commented out because the google news dataset is too large to fit on git\n",
    "#model = gensim.models.KeyedVectors.load_word2vec_format('..//datasets/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking vector size of GoogleNews Word2Vec\n",
    "model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..//datasets/tweets_lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'block', 'of', 'ne', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'se', 'nd', 'ave', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>michellebot</td>\n",
       "      <td>['valley', 'of', 'fire', 'valley', 'of', 'fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'block', 'of', 'ne', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>houdini</td>\n",
       "      <td>['special', 'shout', 'out', 'to', 'm', 'llyk',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1              username  \\\n",
       "0           0             0  mult co fire ems log   \n",
       "1           1             1  mult co fire ems log   \n",
       "2           2             2           michellebot   \n",
       "3           3             3  mult co fire ems log   \n",
       "4           4             4               houdini   \n",
       "\n",
       "                                                text  \n",
       "0  ['med', 'medical', 'at', 'block', 'of', 'ne', ...  \n",
       "1  ['med', 'medical', 'at', 'se', 'nd', 'ave', 's...  \n",
       "2  ['valley', 'of', 'fire', 'valley', 'of', 'fire...  \n",
       "3  ['med', 'medical', 'at', 'block', 'of', 'ne', ...  \n",
       "4  ['special', 'shout', 'out', 'to', 'm', 'llyk',...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning messages to ensure that they are adequately prepped for analysis \n",
    "def message_to_words(raw_message):\n",
    "    \n",
    "     # remove accents\n",
    "    unaccented = unidecode.unidecode(raw_message)\n",
    "    \n",
    "    # remove all non-letter characters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", unaccented)\n",
    "    \n",
    "    # lowercase \n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lem = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "    # stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # return as a string \n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the message...\n",
      "Comment 100 of 11784.\n",
      "Comment 200 of 11784.\n",
      "Comment 300 of 11784.\n",
      "Comment 400 of 11784.\n",
      "Comment 500 of 11784.\n",
      "Comment 600 of 11784.\n",
      "Comment 700 of 11784.\n",
      "Comment 800 of 11784.\n",
      "Comment 900 of 11784.\n",
      "Comment 1000 of 11784.\n",
      "Comment 1100 of 11784.\n",
      "Comment 1200 of 11784.\n",
      "Comment 1300 of 11784.\n",
      "Comment 1400 of 11784.\n",
      "Comment 1500 of 11784.\n",
      "Comment 1600 of 11784.\n",
      "Comment 1700 of 11784.\n",
      "Comment 1800 of 11784.\n",
      "Comment 1900 of 11784.\n",
      "Comment 2000 of 11784.\n",
      "Comment 2100 of 11784.\n",
      "Comment 2200 of 11784.\n",
      "Comment 2300 of 11784.\n",
      "Comment 2400 of 11784.\n",
      "Comment 2500 of 11784.\n",
      "Comment 2600 of 11784.\n",
      "Comment 2700 of 11784.\n",
      "Comment 2800 of 11784.\n",
      "Comment 2900 of 11784.\n",
      "Comment 3000 of 11784.\n",
      "Comment 3100 of 11784.\n",
      "Comment 3200 of 11784.\n",
      "Comment 3300 of 11784.\n",
      "Comment 3400 of 11784.\n",
      "Comment 3500 of 11784.\n",
      "Comment 3600 of 11784.\n",
      "Comment 3700 of 11784.\n",
      "Comment 3800 of 11784.\n",
      "Comment 3900 of 11784.\n",
      "Comment 4000 of 11784.\n",
      "Comment 4100 of 11784.\n",
      "Comment 4200 of 11784.\n",
      "Comment 4300 of 11784.\n",
      "Comment 4400 of 11784.\n",
      "Comment 4500 of 11784.\n",
      "Comment 4600 of 11784.\n",
      "Comment 4700 of 11784.\n",
      "Comment 4800 of 11784.\n",
      "Comment 4900 of 11784.\n",
      "Comment 5000 of 11784.\n",
      "Comment 5100 of 11784.\n",
      "Comment 5200 of 11784.\n",
      "Comment 5300 of 11784.\n",
      "Comment 5400 of 11784.\n",
      "Comment 5500 of 11784.\n",
      "Comment 5600 of 11784.\n",
      "Comment 5700 of 11784.\n",
      "Comment 5800 of 11784.\n",
      "Comment 5900 of 11784.\n",
      "Comment 6000 of 11784.\n",
      "Comment 6100 of 11784.\n",
      "Comment 6200 of 11784.\n",
      "Comment 6300 of 11784.\n",
      "Comment 6400 of 11784.\n",
      "Comment 6500 of 11784.\n",
      "Comment 6600 of 11784.\n",
      "Comment 6700 of 11784.\n",
      "Comment 6800 of 11784.\n",
      "Comment 6900 of 11784.\n",
      "Comment 7000 of 11784.\n",
      "Comment 7100 of 11784.\n",
      "Comment 7200 of 11784.\n",
      "Comment 7300 of 11784.\n",
      "Comment 7400 of 11784.\n",
      "Comment 7500 of 11784.\n",
      "Comment 7600 of 11784.\n",
      "Comment 7700 of 11784.\n",
      "Comment 7800 of 11784.\n",
      "Comment 7900 of 11784.\n",
      "Comment 8000 of 11784.\n",
      "Comment 8100 of 11784.\n",
      "Comment 8200 of 11784.\n",
      "Comment 8300 of 11784.\n",
      "Comment 8400 of 11784.\n",
      "Comment 8500 of 11784.\n",
      "Comment 8600 of 11784.\n",
      "Comment 8700 of 11784.\n",
      "Comment 8800 of 11784.\n",
      "Comment 8900 of 11784.\n",
      "Comment 9000 of 11784.\n",
      "Comment 9100 of 11784.\n",
      "Comment 9200 of 11784.\n",
      "Comment 9300 of 11784.\n",
      "Comment 9400 of 11784.\n",
      "Comment 9500 of 11784.\n",
      "Comment 9600 of 11784.\n",
      "Comment 9700 of 11784.\n",
      "Comment 9800 of 11784.\n",
      "Comment 9900 of 11784.\n",
      "Comment 10000 of 11784.\n",
      "Comment 10100 of 11784.\n",
      "Comment 10200 of 11784.\n",
      "Comment 10300 of 11784.\n",
      "Comment 10400 of 11784.\n",
      "Comment 10500 of 11784.\n",
      "Comment 10600 of 11784.\n",
      "Comment 10700 of 11784.\n",
      "Comment 10800 of 11784.\n",
      "Comment 10900 of 11784.\n",
      "Comment 11000 of 11784.\n",
      "Comment 11100 of 11784.\n",
      "Comment 11200 of 11784.\n",
      "Comment 11300 of 11784.\n",
      "Comment 11400 of 11784.\n",
      "Comment 11500 of 11784.\n",
      "Comment 11600 of 11784.\n",
      "Comment 11700 of 11784.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "total_message = df.shape[0]\n",
    "clean_message = []\n",
    "\n",
    "print(\"Cleaning and parsing the message...\")\n",
    "\n",
    "j = 0\n",
    "for message in df['text']:\n",
    "    clean_message.append(message_to_words(message))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message\n",
    "    if (j+1) % 100 == 0:\n",
    "        print(f'Comment {j+1} of {total_message}.')\n",
    "    \n",
    "    j += 1\n",
    "    \n",
    "    if j == total_message:\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'block', 'of', 'ne', ...</td>\n",
       "      <td>med medical block ne th ave portland portland ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'se', 'nd', 'ave', 's...</td>\n",
       "      <td>med medical se nd ave se johnson creek blvd po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>michellebot</td>\n",
       "      <td>['valley', 'of', 'fire', 'valley', 'of', 'fire...</td>\n",
       "      <td>valley fire valley fire state park</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1              username  \\\n",
       "0           0             0  mult co fire ems log   \n",
       "1           1             1  mult co fire ems log   \n",
       "2           2             2           michellebot   \n",
       "\n",
       "                                                text  \\\n",
       "0  ['med', 'medical', 'at', 'block', 'of', 'ne', ...   \n",
       "1  ['med', 'medical', 'at', 'se', 'nd', 'ave', 's...   \n",
       "2  ['valley', 'of', 'fire', 'valley', 'of', 'fire...   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  med medical block ne th ave portland portland ...  \n",
       "1  med medical se nd ave se johnson creek blvd po...  \n",
       "2                 valley fire valley fire state park  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.assign(cleaned_message = clean_message)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11784, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'username', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11784, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Urgent vs. non-Urgent Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpful medium post**\n",
    "https://medium.com/@belen.sanchez27/leveraging-social-media-to-map-disasters-74b4cc34848d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit to NYC III group (Andrew Sternick, Nick Read, Preeya Sawadmanod), their code provided a better \n",
    "#visualization of the cosine similarity scores for each message vector so when I started to have issues \n",
    "#I found their code and applied it while I am tweeting my bags of words for a clearer understanding of the \n",
    "#cosine similarity scores \n",
    "\n",
    "\n",
    "#Function for vectorization of corpus \n",
    "def twitter_vector(vector):\n",
    "    \n",
    "    #Counter for number of words in corpus that exists in GoogleNews word list \n",
    "    count=0\n",
    "   \n",
    "    #Creating a template for cumulative corpus vector sum (300 dimensions)\n",
    "    tweet_vector = np.zeros((1,300))\n",
    "    \n",
    "    #Iterating over each word in bag of word list \n",
    "    for word in vector:\n",
    "\n",
    "        #Checking if word exists in GoogleNews word list\n",
    "        if word in model.vocab:                    \n",
    "            \n",
    "            #Vectorizing the word if in list\n",
    "            word_vector = model.word_vector(word)        \n",
    "            \n",
    "            #Updating counter\n",
    "            count += 1\n",
    "            \n",
    "            #Updating cumulative vector sum \n",
    "            tweet_vector = tweet_vector + word_vector \n",
    "\n",
    "    #Computing average vector by taking cumulative vector sum and dividing it by number of words traced\n",
    "    tweet_vector_score = tweet_vector/count\n",
    "    \n",
    "    #Using numpy to squeeze N-dimensional nested array object into a 1-D array \n",
    "    tweet_vector_score = np.squeeze(tweet_vector_score)\n",
    "    \n",
    "    return(tweet_vector_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying function to vectorize both bag of words list\n",
    "urgent_vec = twitter_vector(urgent)\n",
    "nonurgent_vec = twitter_vector(non_urgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urgent_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonurgent_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function for computing cosine similarity score \n",
    "def cos_sim_score(a,b): \n",
    "    \n",
    "    #Calculating the dot product of two vectors\n",
    "    dot = np.dot(a, b)\n",
    "    \n",
    "    #Calculating the magnitude of the vector \n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    \n",
    "    #Calculating cosine similarity\n",
    "    cos = dot / (norma * normb)\n",
    "    \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original copy - DO NOT CHANGE\n",
    "#Empty lists\n",
    "urgent_cos_sim = []\n",
    "non_urgent_cos_sim = []\n",
    "\n",
    "#Iterating through each tweet in out list of tokens\n",
    "for tweet in df['cleaned_message']:\n",
    "    \n",
    "    #Call function \n",
    "    avg_vec_per_tweet = tweet_vector(tweet)\n",
    "    \n",
    "    #Creating new column in df for cosine simarlity score \n",
    "    urgent_cos_sim.append(cos_sim_score(avg_vec_per_tweet, urgent_vec))\n",
    "    non_urgent_cos_sim.append(cos_sim_score(avg_vec_per_tweet, nonurgent_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>urgent_cos_sim</th>\n",
       "      <th>non_urgent_cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>med medical block ne th ave portland portland ...</td>\n",
       "      <td>0.148701</td>\n",
       "      <td>0.137452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>med medical se nd ave se johnson creek blvd po...</td>\n",
       "      <td>0.149998</td>\n",
       "      <td>0.135317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>valley fire valley fire state park</td>\n",
       "      <td>0.144539</td>\n",
       "      <td>0.141861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>med medical block ne rodney ave portland portl...</td>\n",
       "      <td>0.145074</td>\n",
       "      <td>0.131240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>special shout llyk comin fire video downtown l...</td>\n",
       "      <td>0.144783</td>\n",
       "      <td>0.133166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     cleaned_message  urgent_cos_sim  \\\n",
       "0  med medical block ne th ave portland portland ...        0.148701   \n",
       "1  med medical se nd ave se johnson creek blvd po...        0.149998   \n",
       "2                 valley fire valley fire state park        0.144539   \n",
       "3  med medical block ne rodney ave portland portl...        0.145074   \n",
       "4  special shout llyk comin fire video downtown l...        0.144783   \n",
       "\n",
       "   non_urgent_cos_sim  \n",
       "0            0.137452  \n",
       "1            0.135317  \n",
       "2            0.141861  \n",
       "3            0.131240  \n",
       "4            0.133166  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['urgent_cos_sim'] = urgent_cos_sim\n",
    "df['non_urgent_cos_sim'] = non_urgent_cos_sim\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Classification \n",
    "urgent = []\n",
    "\n",
    "for rows in df.index:  \n",
    "    if df['urgent_cos_sim'][rows] > df['non_urgent_cos_sim'][rows]:\n",
    "        urgent.append(1)\n",
    "    else: \n",
    "        urgent.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new column\n",
    "df['urgent'] = urgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.64927\n",
       "0    0.35073\n",
       "Name: urgent, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['urgent'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>urgent_cos_sim</th>\n",
       "      <th>non_urgent_cos_sim</th>\n",
       "      <th>urgent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smokea smoke investigation outside structure n...</td>\n",
       "      <td>0.158059</td>\n",
       "      <td>0.164721</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>med medical se nd ave se morrison st morrison ...</td>\n",
       "      <td>0.141777</td>\n",
       "      <td>0.144964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>smokea smoke investigation outside structure s...</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.155813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>buy two get one free decorate tree ghoulish de...</td>\n",
       "      <td>0.142310</td>\n",
       "      <td>0.147313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bay area thankful rain clean air fire brutal n...</td>\n",
       "      <td>0.142018</td>\n",
       "      <td>0.143126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>smokea smoke investigation outside structure n...</td>\n",
       "      <td>0.158059</td>\n",
       "      <td>0.164721</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>med medical se nd ave se morrison st morrison ...</td>\n",
       "      <td>0.141777</td>\n",
       "      <td>0.144964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>smokea smoke investigation outside structure s...</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.155813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>buy two get one free decorate tree ghoulish de...</td>\n",
       "      <td>0.142310</td>\n",
       "      <td>0.147313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bay area thankful rain clean air fire brutal n...</td>\n",
       "      <td>0.142018</td>\n",
       "      <td>0.143126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>want smoke linkinbio zeroaliasmusic thecoldest...</td>\n",
       "      <td>0.139762</td>\n",
       "      <td>0.140521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>today grateful relief fire smoke shot one clea...</td>\n",
       "      <td>0.133591</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>smoke smoke investigation inside structure blo...</td>\n",
       "      <td>0.154809</td>\n",
       "      <td>0.154943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>rfire residential structure fire block se th a...</td>\n",
       "      <td>0.144933</td>\n",
       "      <td>0.148972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>smokea smoke investigation outside structure s...</td>\n",
       "      <td>0.153651</td>\n",
       "      <td>0.159442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>grateful support calfire pio team chief pio of...</td>\n",
       "      <td>0.149404</td>\n",
       "      <td>0.150706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>mineralfire unofficial new fire report detail ...</td>\n",
       "      <td>0.140783</td>\n",
       "      <td>0.144771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>pet paradise syntheticgrass green savingwater ...</td>\n",
       "      <td>0.140078</td>\n",
       "      <td>0.141398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>rain hinders search help nearly put camp fire</td>\n",
       "      <td>0.139970</td>\n",
       "      <td>0.140032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>amazing creature standing last year quite fire...</td>\n",
       "      <td>0.140624</td>\n",
       "      <td>0.145591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cleaned_message  urgent_cos_sim  \\\n",
       "5   smokea smoke investigation outside structure n...        0.158059   \n",
       "9   med medical se nd ave se morrison st morrison ...        0.141777   \n",
       "10  smokea smoke investigation outside structure s...        0.149254   \n",
       "11  buy two get one free decorate tree ghoulish de...        0.142310   \n",
       "12  bay area thankful rain clean air fire brutal n...        0.142018   \n",
       "25  smokea smoke investigation outside structure n...        0.158059   \n",
       "29  med medical se nd ave se morrison st morrison ...        0.141777   \n",
       "30  smokea smoke investigation outside structure s...        0.149254   \n",
       "31  buy two get one free decorate tree ghoulish de...        0.142310   \n",
       "32  bay area thankful rain clean air fire brutal n...        0.142018   \n",
       "44  want smoke linkinbio zeroaliasmusic thecoldest...        0.139762   \n",
       "49  today grateful relief fire smoke shot one clea...        0.133591   \n",
       "52  smoke smoke investigation inside structure blo...        0.154809   \n",
       "53  rfire residential structure fire block se th a...        0.144933   \n",
       "61  smokea smoke investigation outside structure s...        0.153651   \n",
       "65  grateful support calfire pio team chief pio of...        0.149404   \n",
       "66  mineralfire unofficial new fire report detail ...        0.140783   \n",
       "70  pet paradise syntheticgrass green savingwater ...        0.140078   \n",
       "73      rain hinders search help nearly put camp fire        0.139970   \n",
       "78  amazing creature standing last year quite fire...        0.140624   \n",
       "\n",
       "    non_urgent_cos_sim  urgent  \n",
       "5             0.164721       0  \n",
       "9             0.144964       0  \n",
       "10            0.155813       0  \n",
       "11            0.147313       0  \n",
       "12            0.143126       0  \n",
       "25            0.164721       0  \n",
       "29            0.144964       0  \n",
       "30            0.155813       0  \n",
       "31            0.147313       0  \n",
       "32            0.143126       0  \n",
       "44            0.140521       0  \n",
       "49            0.134483       0  \n",
       "52            0.154943       0  \n",
       "53            0.148972       0  \n",
       "61            0.159442       0  \n",
       "65            0.150706       0  \n",
       "66            0.144771       0  \n",
       "70            0.141398       0  \n",
       "73            0.140032       0  \n",
       "78            0.145591       0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view urgent tweets\n",
    "#df[df['urgent'] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the original dataframe to merge back with the raw tweets\n",
    "df_original = pd.read_csv(\"../datasets/raw_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df_original[['text','username']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MED - MEDICAL at 700 BLOCK OF NE 78TH AVE, POR...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MED - MEDICAL at SE 32ND AVE / SE JOHNSON CREE...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valley of Fire @ Valley of Fire State Park htt...</td>\n",
       "      <td>MichelleBot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MED - MEDICAL at 2000 BLOCK OF NE RODNEY AVE, ...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Special shout out to @m0llyk4y for comin throu...</td>\n",
       "      <td>Houdini</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text              username\n",
       "0  MED - MEDICAL at 700 BLOCK OF NE 78TH AVE, POR...  Mult Co Fire/EMS log\n",
       "1  MED - MEDICAL at SE 32ND AVE / SE JOHNSON CREE...  Mult Co Fire/EMS log\n",
       "2  Valley of Fire @ Valley of Fire State Park htt...           MichelleBot\n",
       "3  MED - MEDICAL at 2000 BLOCK OF NE RODNEY AVE, ...  Mult Co Fire/EMS log\n",
       "4  Special shout out to @m0llyk4y for comin throu...               Houdini"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_original, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>urgent_cos_sim</th>\n",
       "      <th>non_urgent_cos_sim</th>\n",
       "      <th>urgent</th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>black friday release sublime greatest hit firs...</td>\n",
       "      <td>0.138924</td>\n",
       "      <td>0.137414</td>\n",
       "      <td>1</td>\n",
       "      <td>with the Black Friday releases of Sublime-Gre...</td>\n",
       "      <td>Programme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749</th>\n",
       "      <td>med medical block ne sandy blvd portland portl...</td>\n",
       "      <td>0.145306</td>\n",
       "      <td>0.128034</td>\n",
       "      <td>1</td>\n",
       "      <td>MED - MEDICAL at 8300 BLOCK OF NE SANDY BLVD, ...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>almcom monitored commercial fire alarm block s...</td>\n",
       "      <td>0.142837</td>\n",
       "      <td>0.145155</td>\n",
       "      <td>0</td>\n",
       "      <td>ALMCOM - MONITORED COMMERCIAL FIRE ALARM at 14...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594</th>\n",
       "      <td>much latergram actually posting copy angievelv...</td>\n",
       "      <td>0.144122</td>\n",
       "      <td>0.143888</td>\n",
       "      <td>1</td>\n",
       "      <td>Much, #latergram I’m actually posting to copy ...</td>\n",
       "      <td>Felicity Kay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10888</th>\n",
       "      <td>smoke campfire thick see bay barely see emeryv...</td>\n",
       "      <td>0.138019</td>\n",
       "      <td>0.139797</td>\n",
       "      <td>0</td>\n",
       "      <td>The smoke from the #CampFire is so thick you c...</td>\n",
       "      <td>Safer at home, Yoda is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         cleaned_message  urgent_cos_sim  \\\n",
       "1353   black friday release sublime greatest hit firs...        0.138924   \n",
       "5749   med medical block ne sandy blvd portland portl...        0.145306   \n",
       "4008   almcom monitored commercial fire alarm block s...        0.142837   \n",
       "4594   much latergram actually posting copy angievelv...        0.144122   \n",
       "10888  smoke campfire thick see bay barely see emeryv...        0.138019   \n",
       "\n",
       "       non_urgent_cos_sim  urgent  \\\n",
       "1353             0.137414       1   \n",
       "5749             0.128034       1   \n",
       "4008             0.145155       0   \n",
       "4594             0.143888       1   \n",
       "10888            0.139797       0   \n",
       "\n",
       "                                                    text  \\\n",
       "1353    with the Black Friday releases of Sublime-Gre...   \n",
       "5749   MED - MEDICAL at 8300 BLOCK OF NE SANDY BLVD, ...   \n",
       "4008   ALMCOM - MONITORED COMMERCIAL FIRE ALARM at 14...   \n",
       "4594   Much, #latergram I’m actually posting to copy ...   \n",
       "10888  The smoke from the #CampFire is so thick you c...   \n",
       "\n",
       "                     username  \n",
       "1353                Programme  \n",
       "5749     Mult Co Fire/EMS log  \n",
       "4008     Mult Co Fire/EMS log  \n",
       "4594             Felicity Kay  \n",
       "10888  Safer at home, Yoda is  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urgent = df.loc[df['urgent'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonurgent = df.loc[df['urgent'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export df to get lat long points \n",
    "df_urgent.to_csv('..//datasets/df_urgent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export df to get lat long points \n",
    "df_nonurgent.to_csv('..//datasets/df_nonurgent.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
