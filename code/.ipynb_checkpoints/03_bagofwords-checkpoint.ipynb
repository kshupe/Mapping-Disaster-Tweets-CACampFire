{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Importing-dataset-of-labeled-diaster-messages\" data-toc-modified-id=\"Importing-dataset-of-labeled-diaster-messages-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Importing dataset of labeled diaster messages</a></span></li><li><span><a href=\"#Processing-Messages\" data-toc-modified-id=\"Processing-Messages-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Processing Messages</a></span></li><li><span><a href=\"#Modelling-Function\" data-toc-modified-id=\"Modelling-Function-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modelling Function</a></span></li><li><span><a href=\"#Modeling-Tests\" data-toc-modified-id=\"Modeling-Tests-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Modeling Tests</a></span></li><li><span><a href=\"#Final-Model\" data-toc-modified-id=\"Final-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Final Model</a></span></li><li><span><a href=\"#Coefficients\" data-toc-modified-id=\"Coefficients-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Coefficients</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#For Modeling\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dataset of labeled diaster messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owner/opt/anaconda3/envs/dsi/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3062: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('..//datasets/disaster_response_messages_training.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>PII</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>train</td>\n",
       "      <td>Information about the National Palace-</td>\n",
       "      <td>Informtion au nivaux palais nationl</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  split                                            message  \\\n",
       "0   2  train  Weather update - a cold front from Cuba that c...   \n",
       "1   7  train            Is the Hurricane over or is it not over   \n",
       "2  12  train  says: west side of Haiti, rest of the country ...   \n",
       "3  14  train             Information about the National Palace-   \n",
       "4  15  train                     Storm at sacred heart of jesus   \n",
       "\n",
       "                                            original   genre  related  PII  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1    0   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1    0   \n",
       "2  facade ouest d Haiti et le reste du pays aujou...  direct        1    0   \n",
       "3                Informtion au nivaux palais nationl  direct        0    0   \n",
       "4                        Cyclone Coeur sacr de jesus  direct        1    0   \n",
       "\n",
       "   request  offer  aid_related  ...  aid_centers  other_infrastructure  \\\n",
       "0        0      0            0  ...            0                     0   \n",
       "1        0      0            1  ...            0                     0   \n",
       "2        0      0            0  ...            0                     0   \n",
       "3        0      0            0  ...            0                     0   \n",
       "4        0      0            0  ...            0                     0   \n",
       "\n",
       "   weather_related  floods  storm  fire  earthquake  cold  other_weather  \\\n",
       "0                0       0      0     0           0     0              0   \n",
       "1                1       0      1     0           0     0              0   \n",
       "2                0       0      0     0           0     0              0   \n",
       "3                0       0      0     0           0     0              0   \n",
       "4                1       0      1     0           0     0              0   \n",
       "\n",
       "   direct_report  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Processing Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def message_to_words(raw_message):\n",
    "    \n",
    "     # remove accents\n",
    "    unaccented = unidecode.unidecode(raw_message)\n",
    "    \n",
    "    # remove all non-letter characters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", unaccented)\n",
    "    \n",
    "    # lowercase \n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lem = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "    # stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # return as a string \n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the message...\n",
      "Comment 100 of 21046.\n",
      "Comment 200 of 21046.\n",
      "Comment 300 of 21046.\n",
      "Comment 400 of 21046.\n",
      "Comment 500 of 21046.\n",
      "Comment 600 of 21046.\n",
      "Comment 700 of 21046.\n",
      "Comment 800 of 21046.\n",
      "Comment 900 of 21046.\n",
      "Comment 1000 of 21046.\n",
      "Comment 1100 of 21046.\n",
      "Comment 1200 of 21046.\n",
      "Comment 1300 of 21046.\n",
      "Comment 1400 of 21046.\n",
      "Comment 1500 of 21046.\n",
      "Comment 1600 of 21046.\n",
      "Comment 1700 of 21046.\n",
      "Comment 1800 of 21046.\n",
      "Comment 1900 of 21046.\n",
      "Comment 2000 of 21046.\n",
      "Comment 2100 of 21046.\n",
      "Comment 2200 of 21046.\n",
      "Comment 2300 of 21046.\n",
      "Comment 2400 of 21046.\n",
      "Comment 2500 of 21046.\n",
      "Comment 2600 of 21046.\n",
      "Comment 2700 of 21046.\n",
      "Comment 2800 of 21046.\n",
      "Comment 2900 of 21046.\n",
      "Comment 3000 of 21046.\n",
      "Comment 3100 of 21046.\n",
      "Comment 3200 of 21046.\n",
      "Comment 3300 of 21046.\n",
      "Comment 3400 of 21046.\n",
      "Comment 3500 of 21046.\n",
      "Comment 3600 of 21046.\n",
      "Comment 3700 of 21046.\n",
      "Comment 3800 of 21046.\n",
      "Comment 3900 of 21046.\n",
      "Comment 4000 of 21046.\n",
      "Comment 4100 of 21046.\n",
      "Comment 4200 of 21046.\n",
      "Comment 4300 of 21046.\n",
      "Comment 4400 of 21046.\n",
      "Comment 4500 of 21046.\n",
      "Comment 4600 of 21046.\n",
      "Comment 4700 of 21046.\n",
      "Comment 4800 of 21046.\n",
      "Comment 4900 of 21046.\n",
      "Comment 5000 of 21046.\n",
      "Comment 5100 of 21046.\n",
      "Comment 5200 of 21046.\n",
      "Comment 5300 of 21046.\n",
      "Comment 5400 of 21046.\n",
      "Comment 5500 of 21046.\n",
      "Comment 5600 of 21046.\n",
      "Comment 5700 of 21046.\n",
      "Comment 5800 of 21046.\n",
      "Comment 5900 of 21046.\n",
      "Comment 6000 of 21046.\n",
      "Comment 6100 of 21046.\n",
      "Comment 6200 of 21046.\n",
      "Comment 6300 of 21046.\n",
      "Comment 6400 of 21046.\n",
      "Comment 6500 of 21046.\n",
      "Comment 6600 of 21046.\n",
      "Comment 6700 of 21046.\n",
      "Comment 6800 of 21046.\n",
      "Comment 6900 of 21046.\n",
      "Comment 7000 of 21046.\n",
      "Comment 7100 of 21046.\n",
      "Comment 7200 of 21046.\n",
      "Comment 7300 of 21046.\n",
      "Comment 7400 of 21046.\n",
      "Comment 7500 of 21046.\n",
      "Comment 7600 of 21046.\n",
      "Comment 7700 of 21046.\n",
      "Comment 7800 of 21046.\n",
      "Comment 7900 of 21046.\n",
      "Comment 8000 of 21046.\n",
      "Comment 8100 of 21046.\n",
      "Comment 8200 of 21046.\n",
      "Comment 8300 of 21046.\n",
      "Comment 8400 of 21046.\n",
      "Comment 8500 of 21046.\n",
      "Comment 8600 of 21046.\n",
      "Comment 8700 of 21046.\n",
      "Comment 8800 of 21046.\n",
      "Comment 8900 of 21046.\n",
      "Comment 9000 of 21046.\n",
      "Comment 9100 of 21046.\n",
      "Comment 9200 of 21046.\n",
      "Comment 9300 of 21046.\n",
      "Comment 9400 of 21046.\n",
      "Comment 9500 of 21046.\n",
      "Comment 9600 of 21046.\n",
      "Comment 9700 of 21046.\n",
      "Comment 9800 of 21046.\n",
      "Comment 9900 of 21046.\n",
      "Comment 10000 of 21046.\n",
      "Comment 10100 of 21046.\n",
      "Comment 10200 of 21046.\n",
      "Comment 10300 of 21046.\n",
      "Comment 10400 of 21046.\n",
      "Comment 10500 of 21046.\n",
      "Comment 10600 of 21046.\n",
      "Comment 10700 of 21046.\n",
      "Comment 10800 of 21046.\n",
      "Comment 10900 of 21046.\n",
      "Comment 11000 of 21046.\n",
      "Comment 11100 of 21046.\n",
      "Comment 11200 of 21046.\n",
      "Comment 11300 of 21046.\n",
      "Comment 11400 of 21046.\n",
      "Comment 11500 of 21046.\n",
      "Comment 11600 of 21046.\n",
      "Comment 11700 of 21046.\n",
      "Comment 11800 of 21046.\n",
      "Comment 11900 of 21046.\n",
      "Comment 12000 of 21046.\n",
      "Comment 12100 of 21046.\n",
      "Comment 12200 of 21046.\n",
      "Comment 12300 of 21046.\n",
      "Comment 12400 of 21046.\n",
      "Comment 12500 of 21046.\n",
      "Comment 12600 of 21046.\n",
      "Comment 12700 of 21046.\n",
      "Comment 12800 of 21046.\n",
      "Comment 12900 of 21046.\n",
      "Comment 13000 of 21046.\n",
      "Comment 13100 of 21046.\n",
      "Comment 13200 of 21046.\n",
      "Comment 13300 of 21046.\n",
      "Comment 13400 of 21046.\n",
      "Comment 13500 of 21046.\n",
      "Comment 13600 of 21046.\n",
      "Comment 13700 of 21046.\n",
      "Comment 13800 of 21046.\n",
      "Comment 13900 of 21046.\n",
      "Comment 14000 of 21046.\n",
      "Comment 14100 of 21046.\n",
      "Comment 14200 of 21046.\n",
      "Comment 14300 of 21046.\n",
      "Comment 14400 of 21046.\n",
      "Comment 14500 of 21046.\n",
      "Comment 14600 of 21046.\n",
      "Comment 14700 of 21046.\n",
      "Comment 14800 of 21046.\n",
      "Comment 14900 of 21046.\n",
      "Comment 15000 of 21046.\n",
      "Comment 15100 of 21046.\n",
      "Comment 15200 of 21046.\n",
      "Comment 15300 of 21046.\n",
      "Comment 15400 of 21046.\n",
      "Comment 15500 of 21046.\n",
      "Comment 15600 of 21046.\n",
      "Comment 15700 of 21046.\n",
      "Comment 15800 of 21046.\n",
      "Comment 15900 of 21046.\n",
      "Comment 16000 of 21046.\n",
      "Comment 16100 of 21046.\n",
      "Comment 16200 of 21046.\n",
      "Comment 16300 of 21046.\n",
      "Comment 16400 of 21046.\n",
      "Comment 16500 of 21046.\n",
      "Comment 16600 of 21046.\n",
      "Comment 16700 of 21046.\n",
      "Comment 16800 of 21046.\n",
      "Comment 16900 of 21046.\n",
      "Comment 17000 of 21046.\n",
      "Comment 17100 of 21046.\n",
      "Comment 17200 of 21046.\n",
      "Comment 17300 of 21046.\n",
      "Comment 17400 of 21046.\n",
      "Comment 17500 of 21046.\n",
      "Comment 17600 of 21046.\n",
      "Comment 17700 of 21046.\n",
      "Comment 17800 of 21046.\n",
      "Comment 17900 of 21046.\n",
      "Comment 18000 of 21046.\n",
      "Comment 18100 of 21046.\n",
      "Comment 18200 of 21046.\n",
      "Comment 18300 of 21046.\n",
      "Comment 18400 of 21046.\n",
      "Comment 18500 of 21046.\n",
      "Comment 18600 of 21046.\n",
      "Comment 18700 of 21046.\n",
      "Comment 18800 of 21046.\n",
      "Comment 18900 of 21046.\n",
      "Comment 19000 of 21046.\n",
      "Comment 19100 of 21046.\n",
      "Comment 19200 of 21046.\n",
      "Comment 19300 of 21046.\n",
      "Comment 19400 of 21046.\n",
      "Comment 19500 of 21046.\n",
      "Comment 19600 of 21046.\n",
      "Comment 19700 of 21046.\n",
      "Comment 19800 of 21046.\n",
      "Comment 19900 of 21046.\n",
      "Comment 20000 of 21046.\n",
      "Comment 20100 of 21046.\n",
      "Comment 20200 of 21046.\n",
      "Comment 20300 of 21046.\n",
      "Comment 20400 of 21046.\n",
      "Comment 20500 of 21046.\n",
      "Comment 20600 of 21046.\n",
      "Comment 20700 of 21046.\n",
      "Comment 20800 of 21046.\n",
      "Comment 20900 of 21046.\n",
      "Comment 21000 of 21046.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "total_message = df.shape[0]\n",
    "clean_message = []\n",
    "\n",
    "print(\"Cleaning and parsing the message...\")\n",
    "\n",
    "j = 0\n",
    "for message in df['message']:\n",
    "    clean_message.append(message_to_words(message))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message\n",
    "    if (j+1) % 100 == 0:\n",
    "        print(f'Comment {j+1} of {total_message}.')\n",
    "    \n",
    "    j += 1\n",
    "    \n",
    "    if j == total_message:\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>PII</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>...</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>weather update cold front cuba could pass haiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>train</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>train</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>says west side haiti rest country today tonight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  split                                            message  \\\n",
       "0   2  train  Weather update - a cold front from Cuba that c...   \n",
       "1   7  train            Is the Hurricane over or is it not over   \n",
       "2  12  train  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  related  PII  \\\n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct        1    0   \n",
       "1                 Cyclone nan fini osinon li pa fini  direct        1    0   \n",
       "2  facade ouest d Haiti et le reste du pays aujou...  direct        1    0   \n",
       "\n",
       "   request  offer  aid_related  ...  other_infrastructure  weather_related  \\\n",
       "0        0      0            0  ...                     0                0   \n",
       "1        0      0            1  ...                     0                1   \n",
       "2        0      0            0  ...                     0                0   \n",
       "\n",
       "   floods  storm  fire  earthquake  cold  other_weather  direct_report  \\\n",
       "0       0      0     0           0     0              0              0   \n",
       "1       0      1     0           0     0              0              0   \n",
       "2       0      0     0           0     0              0              0   \n",
       "\n",
       "                                   cleaned_message  \n",
       "0  weather update cold front cuba could pass haiti  \n",
       "1                                        hurricane  \n",
       "2  says west side haiti rest country today tonight  \n",
       "\n",
       "[3 rows x 43 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.assign(cleaned_message = clean_message)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modelling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##Taken from NYC I Example \n",
    "\n",
    "def text_to_model(X_column, model, vectorizer, params, verbose = 1):\n",
    "    \n",
    "    X = df[X_column]                                    #creates X and y\n",
    "    y = df['direct_report']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = .25)    #train test split\n",
    "    \n",
    "    pipe = Pipeline([                               #pipeline to run with gridsearch and test hyperparameters\n",
    "        ('vectorizer', vectorizer),                 #for both the model and vectorizer.\n",
    "        ('model', model)])                          #this will be done for many vectorizer-model combinations\n",
    "        \n",
    "    grid = GridSearchCV(pipe, param_grid=params, cv=5, verbose=verbose)  \n",
    "    \n",
    "    grid.fit(X_train, y_train)                       #fitting the grid model to X_train and y_train and \n",
    "                                                     #running a 5 fold cross validation\n",
    "    score_dict = {}\n",
    "    \n",
    "    score_dict['X'] = X_column                               #this dict will be converted to dataframe to store \n",
    "    score_dict['Vectorizer'] = vectorizer                    #the performance of each gridsearch and return the \n",
    "    score_dict['Model'] = model                              #best parameters and score to compare to other models\n",
    "    score_dict['train_score'] = grid.score(X_train, y_train)\n",
    "    score_dict['test_score'] = grid.score(X_test, y_test)\n",
    "    score_dict['best_params'] = grid.best_params_\n",
    "    \n",
    "    try:\n",
    "        return pd.DataFrame(score_dict)\n",
    "    except:\n",
    "        return score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modeling Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Model</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model__max_iter</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.873163</td>\n",
       "      <td>0.86355</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model__penalty</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.873163</td>\n",
       "      <td>0.86355</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer__max_df</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.873163</td>\n",
       "      <td>0.86355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer__max_features</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.873163</td>\n",
       "      <td>0.86355</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer__min_df</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.873163</td>\n",
       "      <td>0.86355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer__ngram_range</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.873163</td>\n",
       "      <td>0.86355</td>\n",
       "      <td>(1, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        X  \\\n",
       "model__max_iter           cleaned_message   \n",
       "model__penalty            cleaned_message   \n",
       "vectorizer__max_df        cleaned_message   \n",
       "vectorizer__max_features  cleaned_message   \n",
       "vectorizer__min_df        cleaned_message   \n",
       "vectorizer__ngram_range   cleaned_message   \n",
       "\n",
       "                                                                 Vectorizer  \\\n",
       "model__max_iter           CountVectorizer(analyzer='word', binary=False,...   \n",
       "model__penalty            CountVectorizer(analyzer='word', binary=False,...   \n",
       "vectorizer__max_df        CountVectorizer(analyzer='word', binary=False,...   \n",
       "vectorizer__max_features  CountVectorizer(analyzer='word', binary=False,...   \n",
       "vectorizer__min_df        CountVectorizer(analyzer='word', binary=False,...   \n",
       "vectorizer__ngram_range   CountVectorizer(analyzer='word', binary=False,...   \n",
       "\n",
       "                                                                      Model  \\\n",
       "model__max_iter           LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "model__penalty            LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "vectorizer__max_df        LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "vectorizer__max_features  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "vectorizer__min_df        LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "vectorizer__ngram_range   LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "\n",
       "                          train_score  test_score best_params  \n",
       "model__max_iter              0.873163     0.86355        1500  \n",
       "model__penalty               0.873163     0.86355          l2  \n",
       "vectorizer__max_df           0.873163     0.86355           1  \n",
       "vectorizer__max_features     0.873163     0.86355        1000  \n",
       "vectorizer__min_df           0.873163     0.86355           1  \n",
       "vectorizer__ngram_range      0.873163     0.86355      (1, 3)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Selected - Tokenized messsages\n",
    "# Running a gridsearch on Logistic Regression and Count Vectorizer \n",
    "\n",
    "logreg = LogisticRegression()\n",
    "cv = CountVectorizer()\n",
    "\n",
    "params = {\n",
    "          'vectorizer__max_features':[100, 500, 1000, 1500, 5000],\n",
    "          'vectorizer__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "          'vectorizer__min_df':[1, 2, 3, 4],\n",
    "          'vectorizer__max_df':[1.0],\n",
    "          'model__penalty':['l2'],\n",
    "          'model__max_iter': [1500]\n",
    "         }\n",
    "\n",
    "count_vect_logreg = text_to_model('cleaned_message', model = logreg, vectorizer=cv, params=params)\n",
    "count_vect_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Model</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model__max_iter</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.887164</td>\n",
       "      <td>0.870391</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model__penalty</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.887164</td>\n",
       "      <td>0.870391</td>\n",
       "      <td>l2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer__max_df</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.887164</td>\n",
       "      <td>0.870391</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer__max_features</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.887164</td>\n",
       "      <td>0.870391</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer__min_df</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.887164</td>\n",
       "      <td>0.870391</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vectorizer__ngram_range</th>\n",
       "      <td>cleaned_message</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.887164</td>\n",
       "      <td>0.870391</td>\n",
       "      <td>(1, 2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        X  \\\n",
       "model__max_iter           cleaned_message   \n",
       "model__penalty            cleaned_message   \n",
       "vectorizer__max_df        cleaned_message   \n",
       "vectorizer__max_features  cleaned_message   \n",
       "vectorizer__min_df        cleaned_message   \n",
       "vectorizer__ngram_range   cleaned_message   \n",
       "\n",
       "                                                                 Vectorizer  \\\n",
       "model__max_iter           TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "model__penalty            TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "vectorizer__max_df        TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "vectorizer__max_features  TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "vectorizer__min_df        TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "vectorizer__ngram_range   TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "\n",
       "                                                                      Model  \\\n",
       "model__max_iter           LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "model__penalty            LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "vectorizer__max_df        LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "vectorizer__max_features  LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "vectorizer__min_df        LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "vectorizer__ngram_range   LogisticRegression(C=1.0, class_weight=None, d...   \n",
       "\n",
       "                          train_score  test_score best_params  \n",
       "model__max_iter              0.887164    0.870391        1500  \n",
       "model__penalty               0.887164    0.870391          l2  \n",
       "vectorizer__max_df           0.887164    0.870391           1  \n",
       "vectorizer__max_features     0.887164    0.870391       50000  \n",
       "vectorizer__min_df           0.887164    0.870391           3  \n",
       "vectorizer__ngram_range      0.887164    0.870391      (1, 2)  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Selected - Tokenized Message\n",
    "# Gridsearching Hyperparameters for TF-IDF Vectorizer\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "params = {\n",
    "          'vectorizer__max_features':[1000, 1500, 5000, 50000],\n",
    "          'vectorizer__ngram_range':[(1,1), (1,2), (1,3)],\n",
    "          'vectorizer__min_df':[1, 2, 3, 4],\n",
    "          'vectorizer__max_df':[1.0],\n",
    "          'model__penalty':['l2'],\n",
    "          'model__max_iter': [1500]\n",
    "         }\n",
    "\n",
    "tfidf_vect_logreg = text_to_model(X_column='cleaned_message', model = logreg, vectorizer=tfidf, params=params)\n",
    "tfidf_vect_logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = df['cleaned_message']                                   \n",
    "y = df['direct_report']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = .2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#After grid searching I choose to fit and create predictions with my best fit Logistic Regression model\n",
    "vec = TfidfVectorizer({ 'max_features':[50000],\n",
    "                       'ngram_range':(1,2),\n",
    "                       'min_df':[3],\n",
    "                       'max_df':[1.0]})\n",
    "\n",
    "# Scale data with vectorizer\n",
    "X_train_vec = vec.fit_transform(X_train)\n",
    "X_train_vec = pd.DataFrame(X_train_vec.toarray(), columns=vec.get_feature_names())\n",
    "\n",
    "# Transform the test set\n",
    "X_test_vec = vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.8601802142776014\n",
      "Training Score: 0.8825136612021858\n",
      "Testing Score: 0.8669833729216152\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = LogisticRegression(solver='lbfgs', penalty = 'l2', max_iter = 1500)\n",
    "\n",
    "# Fit on training data.\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Get scores\n",
    "print('CV score:', cross_val_score(model, X_train_vec, y_train, cv=5).mean())\n",
    "print('Training Score:', model.score(X_train_vec, y_train))\n",
    "print('Testing Score:', model.score(X_test_vec, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predneg</th>\n",
       "      <th>pred pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual neg</th>\n",
       "      <td>3335</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual pos</th>\n",
       "      <td>432</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predneg  pred pos\n",
       "actual neg     3335       128\n",
       "actual pos      432       315"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df = pd.DataFrame(cm, columns=['predneg', 'pred pos'], index=['actual neg', 'actual pos'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.6983%\n",
      "Misclassification rate: 13.3017%\n",
      "Sensitivity: 42.1687%\n",
      "Specificity: 96.3038%\n",
      "Precision: 71.1061%\n",
      "ROC AUC: 0.6924\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fn + fp + tn) * 100\n",
    "misclassification = (100 - accuracy)\n",
    "sensitivity = tp / (tp + fn) * 100\n",
    "specificity = tn / (tn + fp) * 100\n",
    "precision = tp / (tp + fp) * 100\n",
    "roc_auc = roc_auc_score(y_test, pred)\n",
    "\n",
    "print(f'Accuracy: {round(accuracy, 4)}%')\n",
    "print(f'Misclassification rate: {round(misclassification, 4)}%')\n",
    "print(f'Sensitivity: {round(sensitivity, 4)}%')\n",
    "print(f'Specificity: {round(specificity, 4)}%')\n",
    "print(f'Precision: {round(precision, 4)}%')\n",
    "print(f'ROC AUC: {round(roc_auc, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Create dataframe with coefs and e^coefs for each word\n",
    "\n",
    "coefs = list(zip(vec.get_feature_names(), model.coef_[0].T))\n",
    "coefs = pd.DataFrame(coefs, columns = ['word','coef'])\n",
    "coefs['e^coef'] = np.exp(coefs['coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coef</th>\n",
       "      <th>e^coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15732</th>\n",
       "      <td>need</td>\n",
       "      <td>3.975140</td>\n",
       "      <td>53.257554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24916</th>\n",
       "      <td>us</td>\n",
       "      <td>3.909328</td>\n",
       "      <td>49.865451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8736</th>\n",
       "      <td>food</td>\n",
       "      <td>3.879859</td>\n",
       "      <td>48.417376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10393</th>\n",
       "      <td>help</td>\n",
       "      <td>3.861895</td>\n",
       "      <td>47.555405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10820</th>\n",
       "      <td>hungry</td>\n",
       "      <td>3.852896</td>\n",
       "      <td>47.129349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>aid</td>\n",
       "      <td>3.464018</td>\n",
       "      <td>31.945077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23387</th>\n",
       "      <td>tents</td>\n",
       "      <td>3.457433</td>\n",
       "      <td>31.735403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23380</th>\n",
       "      <td>tent</td>\n",
       "      <td>3.442254</td>\n",
       "      <td>31.257317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20494</th>\n",
       "      <td>sandy</td>\n",
       "      <td>3.167935</td>\n",
       "      <td>23.758382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25568</th>\n",
       "      <td>water</td>\n",
       "      <td>3.164311</td>\n",
       "      <td>23.672426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10712</th>\n",
       "      <td>house</td>\n",
       "      <td>3.091979</td>\n",
       "      <td>22.020605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6989</th>\n",
       "      <td>dying</td>\n",
       "      <td>3.000751</td>\n",
       "      <td>20.100621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22289</th>\n",
       "      <td>starving</td>\n",
       "      <td>2.673388</td>\n",
       "      <td>14.488977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10818</th>\n",
       "      <td>hunger</td>\n",
       "      <td>2.633808</td>\n",
       "      <td>13.926696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>area</td>\n",
       "      <td>2.572726</td>\n",
       "      <td>13.101496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18957</th>\n",
       "      <td>rain</td>\n",
       "      <td>2.520618</td>\n",
       "      <td>12.436275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25187</th>\n",
       "      <td>victim</td>\n",
       "      <td>2.444524</td>\n",
       "      <td>11.525062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19217</th>\n",
       "      <td>received</td>\n",
       "      <td>2.400962</td>\n",
       "      <td>11.033786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25236</th>\n",
       "      <td>village</td>\n",
       "      <td>2.373490</td>\n",
       "      <td>10.734796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10609</th>\n",
       "      <td>home</td>\n",
       "      <td>2.335617</td>\n",
       "      <td>10.335836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13578</th>\n",
       "      <td>live</td>\n",
       "      <td>2.305556</td>\n",
       "      <td>10.029749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13736</th>\n",
       "      <td>lot</td>\n",
       "      <td>2.235205</td>\n",
       "      <td>9.348396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10840</th>\n",
       "      <td>hurricane</td>\n",
       "      <td>2.151342</td>\n",
       "      <td>8.596385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21264</th>\n",
       "      <td>shelter</td>\n",
       "      <td>2.115939</td>\n",
       "      <td>8.297371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22481</th>\n",
       "      <td>street</td>\n",
       "      <td>2.089527</td>\n",
       "      <td>8.081095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>road</td>\n",
       "      <td>2.086716</td>\n",
       "      <td>8.058409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13735</th>\n",
       "      <td>lost</td>\n",
       "      <td>2.044283</td>\n",
       "      <td>7.723619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2969</th>\n",
       "      <td>broken</td>\n",
       "      <td>1.998262</td>\n",
       "      <td>7.376227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5853</th>\n",
       "      <td>delmas</td>\n",
       "      <td>1.995137</td>\n",
       "      <td>7.353210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21892</th>\n",
       "      <td>something</td>\n",
       "      <td>1.994093</td>\n",
       "      <td>7.345536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word      coef     e^coef\n",
       "15732       need  3.975140  53.257554\n",
       "24916         us  3.909328  49.865451\n",
       "8736        food  3.879859  48.417376\n",
       "10393       help  3.861895  47.555405\n",
       "10820     hungry  3.852896  47.129349\n",
       "540          aid  3.464018  31.945077\n",
       "23387      tents  3.457433  31.735403\n",
       "23380       tent  3.442254  31.257317\n",
       "20494      sandy  3.167935  23.758382\n",
       "25568      water  3.164311  23.672426\n",
       "10712      house  3.091979  22.020605\n",
       "6989       dying  3.000751  20.100621\n",
       "22289   starving  2.673388  14.488977\n",
       "10818     hunger  2.633808  13.926696\n",
       "1261        area  2.572726  13.101496\n",
       "18957       rain  2.520618  12.436275\n",
       "25187     victim  2.444524  11.525062\n",
       "19217   received  2.400962  11.033786\n",
       "25236    village  2.373490  10.734796\n",
       "10609       home  2.335617  10.335836\n",
       "13578       live  2.305556  10.029749\n",
       "13736        lot  2.235205   9.348396\n",
       "10840  hurricane  2.151342   8.596385\n",
       "21264    shelter  2.115939   8.297371\n",
       "22481     street  2.089527   8.081095\n",
       "20051       road  2.086716   8.058409\n",
       "13735       lost  2.044283   7.723619\n",
       "2969      broken  1.998262   7.376227\n",
       "5853      delmas  1.995137   7.353210\n",
       "21892  something  1.994093   7.345536"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.sort_values(by='e^coef', ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coef</th>\n",
       "      <th>e^coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12175</th>\n",
       "      <td>job</td>\n",
       "      <td>-3.166459</td>\n",
       "      <td>0.042153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11438</th>\n",
       "      <td>information</td>\n",
       "      <td>-2.393985</td>\n",
       "      <td>0.091265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20379</th>\n",
       "      <td>said</td>\n",
       "      <td>-2.215541</td>\n",
       "      <td>0.109094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11278</th>\n",
       "      <td>including</td>\n",
       "      <td>-1.900485</td>\n",
       "      <td>0.149496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11694</th>\n",
       "      <td>international</td>\n",
       "      <td>-1.862094</td>\n",
       "      <td>0.155347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16093</th>\n",
       "      <td>notes</td>\n",
       "      <td>-1.743793</td>\n",
       "      <td>0.174856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15844</th>\n",
       "      <td>news</td>\n",
       "      <td>-1.737536</td>\n",
       "      <td>0.175953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9632</th>\n",
       "      <td>government</td>\n",
       "      <td>-1.717573</td>\n",
       "      <td>0.179501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11287</th>\n",
       "      <td>incomplete</td>\n",
       "      <td>-1.700484</td>\n",
       "      <td>0.182595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5128</th>\n",
       "      <td>country</td>\n",
       "      <td>-1.635778</td>\n",
       "      <td>0.194801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23622</th>\n",
       "      <td>thousands</td>\n",
       "      <td>-1.527018</td>\n",
       "      <td>0.217182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19659</th>\n",
       "      <td>reported</td>\n",
       "      <td>-1.483239</td>\n",
       "      <td>0.226902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11439</th>\n",
       "      <td>informations</td>\n",
       "      <td>-1.386566</td>\n",
       "      <td>0.249932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14338</th>\n",
       "      <td>materials</td>\n",
       "      <td>-1.363976</td>\n",
       "      <td>0.255642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25992</th>\n",
       "      <td>work</td>\n",
       "      <td>-1.361786</td>\n",
       "      <td>0.256203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26233</th>\n",
       "      <td>year</td>\n",
       "      <td>-1.291173</td>\n",
       "      <td>0.274948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14819</th>\n",
       "      <td>million</td>\n",
       "      <td>-1.268201</td>\n",
       "      <td>0.281337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17177</th>\n",
       "      <td>passport</td>\n",
       "      <td>-1.240541</td>\n",
       "      <td>0.289228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18298</th>\n",
       "      <td>price</td>\n",
       "      <td>-1.183868</td>\n",
       "      <td>0.306092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21003</th>\n",
       "      <td>senegal</td>\n",
       "      <td>-1.182113</td>\n",
       "      <td>0.306630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word      coef    e^coef\n",
       "12175            job -3.166459  0.042153\n",
       "11438    information -2.393985  0.091265\n",
       "20379           said -2.215541  0.109094\n",
       "11278      including -1.900485  0.149496\n",
       "11694  international -1.862094  0.155347\n",
       "16093          notes -1.743793  0.174856\n",
       "15844           news -1.737536  0.175953\n",
       "9632      government -1.717573  0.179501\n",
       "11287     incomplete -1.700484  0.182595\n",
       "5128         country -1.635778  0.194801\n",
       "23622      thousands -1.527018  0.217182\n",
       "19659       reported -1.483239  0.226902\n",
       "11439   informations -1.386566  0.249932\n",
       "14338      materials -1.363976  0.255642\n",
       "25992           work -1.361786  0.256203\n",
       "26233           year -1.291173  0.274948\n",
       "14819        million -1.268201  0.281337\n",
       "17177       passport -1.240541  0.289228\n",
       "18298          price -1.183868  0.306092\n",
       "21003        senegal -1.182113  0.306630"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.sort_values(by='e^coef').head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
