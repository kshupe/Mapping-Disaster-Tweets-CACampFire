{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Bag-of-Words\" data-toc-modified-id=\"Bag-of-Words-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bag of Words</a></span></li><li><span><a href=\"#Train-Words2Vec\" data-toc-modified-id=\"Train-Words2Vec-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train Words2Vec</a></span></li><li><span><a href=\"#Importing-the-tweets\" data-toc-modified-id=\"Importing-the-tweets-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Importing the tweets</a></span></li><li><span><a href=\"#Creating-the-Urgent-vs.-non-Urgent-Vectors\" data-toc-modified-id=\"Creating-the-Urgent-vs.-non-Urgent-Vectors-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Creating the Urgent vs. non-Urgent Vectors</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **words were created using a logistic regression on an open source database used for NLP that contains messages during disasters and their categorizations, one of those being if the message was a direct call for help, what we will classify as a message being more \"urgent\"**\n",
    "    - https://appen.com/datasets/combined-disaster-response-data/\n",
    "    \n",
    "    \n",
    "- **another way to classify is a message is more urgent is using FEMA's Public Assistance Program and Policy Guide**\n",
    "    - in this guide, public assistance work is either emergency work or permanent work. Important terms from each designation were added to our bag of words \n",
    "    - **Emergency**: \n",
    "        - Emergency Protective Measures\n",
    "        - Debris Removal \n",
    "    - **Permanent**:\n",
    "        - Roads and Bridges\n",
    "        - Water Control Facilities\n",
    "        - Buildings and Equipment\n",
    "        - Utilities \n",
    "        - Parks, Recreational, other \n",
    "        \n",
    "    - https://www.fema.gov/media-library-data/1525468328389-4a038bbef9081cd7dfe7538e7751aa9c/PAPPG_3.1_508_FINAL_5-4-2018.pdf\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of urgent words\n",
    "urgent = ['help',\n",
    "          'campfire',\n",
    "          'tonight',\n",
    "          'today',\n",
    "          'fire',\n",
    "          'need',\n",
    "          #'hungry',\n",
    "          'aid',\n",
    "          'removal',\n",
    "          #'forest',\n",
    "          #'wood',\n",
    "          #'inhalation',\n",
    "          #'dark',\n",
    "          #'smokey',\n",
    "          #'breathe',\n",
    "          'burn',\n",
    "          'tree',\n",
    "          #'yard',\n",
    "          #'garage',\n",
    "          #creek',\n",
    "          #'tent',\n",
    "          #'dying',\n",
    "          'evacuation',\n",
    "          #'starving',\n",
    "          'smoke',\n",
    "          #'shelter',\n",
    "          #'debris',\n",
    "          'unsafe',\n",
    "          #'access',\n",
    "          #'rescue',\n",
    "          'search',\n",
    "          'lost',\n",
    "          'victim',\n",
    "          #'sos', \n",
    "          'medical',\n",
    "          'med',\n",
    "          'urgent',\n",
    "          #'quick',\n",
    "          'home',\n",
    "          #'waste',\n",
    "          #'junk',\n",
    "          #'ditch',\n",
    "          #'seen',\n",
    "          'haze',\n",
    "          #'now',\n",
    "          #'in',\n",
    "          'serious',\n",
    "          #'deadly',\n",
    "          #'report',\n",
    "          'fires',\n",
    "          'reporting',\n",
    "          'smoking',\n",
    "          'come',\n",
    "          #'quick',\n",
    "          #'observe',\n",
    "          #'surge',\n",
    "          #'blaze',\n",
    "          #'ems',\n",
    "          'emergency',\n",
    "          'wildfire',\n",
    "          'Portland',\n",
    "          'send',\n",
    "          #'locate',\n",
    "          'park',\n",
    "          'valley',\n",
    "          'investigation',\n",
    "          #'ave',\n",
    "          #'st',\n",
    "          'structure'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of non-urgent words \n",
    "non_urgent = ['job',\n",
    "              'news',\n",
    "              'government',\n",
    "              'country',\n",
    "              #'reported',\n",
    "              'materials',\n",
    "              'work',\n",
    "              'price',\n",
    "              'utilities',\n",
    "              'facility',\n",
    "              #'st',\n",
    "              #'ave',\n",
    "              #'parks',\n",
    "              #'playground',\n",
    "              #'bridge',\n",
    "              #'sidewalk',\n",
    "              #'guardrails',\n",
    "              'erosion',\n",
    "              'irrigation',\n",
    "              #'baseball',\n",
    "              #'tennis',\n",
    "              #'hiking',\n",
    "              #'wildlife',\n",
    "              #'vegetation',\n",
    "              #'traffic',\n",
    "              'restoration',\n",
    "              'shoulder',\n",
    "              #'stabilization',\n",
    "              #'inspection',\n",
    "              #'assessment',\n",
    "              #'remediation',\n",
    "              #'insurance',\n",
    "              #'cops'\n",
    "              #'help',\n",
    "              'mud',\n",
    "              'silt',\n",
    "              'ditch',\n",
    "              'slip',\n",
    "              #'downtown',\n",
    "              #'uptown',\n",
    "              'pray',\n",
    "              'hope',\n",
    "              'thanks',\n",
    "              'thankful'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Words2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = gensim.models.KeyedVectors.load_word2vec_format('..//datasets/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking vector size of GoogleNews Word2Vec\n",
    "model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.35546875,  0.18359375,  0.14941406, -0.09375   ,  0.17871094,\n",
       "       -0.08398438, -0.06396484, -0.2578125 , -0.1171875 ,  0.13671875,\n",
       "        0.22753906, -0.25585938, -0.18554688, -0.20800781, -0.17089844,\n",
       "        0.02563477, -0.12011719, -0.10839844, -0.06347656,  0.01519775],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example for word rescue\n",
    "fire_vec = model['fire']\n",
    "#Checking for 20 components\n",
    "fire_vec[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..//datasets/tweets_lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'block', 'of', 'ne', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'se', 'nd', 'ave', 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>michellebot</td>\n",
       "      <td>['valley', 'of', 'fire', 'valley', 'of', 'fire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'block', 'of', 'ne', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>houdini</td>\n",
       "      <td>['special', 'shout', 'out', 'to', 'm', 'llyk',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1              username  \\\n",
       "0           0             0  mult co fire ems log   \n",
       "1           1             1  mult co fire ems log   \n",
       "2           2             2           michellebot   \n",
       "3           3             3  mult co fire ems log   \n",
       "4           4             4               houdini   \n",
       "\n",
       "                                                text  \n",
       "0  ['med', 'medical', 'at', 'block', 'of', 'ne', ...  \n",
       "1  ['med', 'medical', 'at', 'se', 'nd', 'ave', 's...  \n",
       "2  ['valley', 'of', 'fire', 'valley', 'of', 'fire...  \n",
       "3  ['med', 'medical', 'at', 'block', 'of', 'ne', ...  \n",
       "4  ['special', 'shout', 'out', 'to', 'm', 'llyk',...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_to_words(raw_message):\n",
    "    \n",
    "     # remove accents\n",
    "    unaccented = unidecode.unidecode(raw_message)\n",
    "    \n",
    "    # remove all non-letter characters\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", unaccented)\n",
    "    \n",
    "    # lowercase \n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens_lem = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "    # stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    \n",
    "    # return as a string \n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the message...\n",
      "Comment 100 of 11784.\n",
      "Comment 200 of 11784.\n",
      "Comment 300 of 11784.\n",
      "Comment 400 of 11784.\n",
      "Comment 500 of 11784.\n",
      "Comment 600 of 11784.\n",
      "Comment 700 of 11784.\n",
      "Comment 800 of 11784.\n",
      "Comment 900 of 11784.\n",
      "Comment 1000 of 11784.\n",
      "Comment 1100 of 11784.\n",
      "Comment 1200 of 11784.\n",
      "Comment 1300 of 11784.\n",
      "Comment 1400 of 11784.\n",
      "Comment 1500 of 11784.\n",
      "Comment 1600 of 11784.\n",
      "Comment 1700 of 11784.\n",
      "Comment 1800 of 11784.\n",
      "Comment 1900 of 11784.\n",
      "Comment 2000 of 11784.\n",
      "Comment 2100 of 11784.\n",
      "Comment 2200 of 11784.\n",
      "Comment 2300 of 11784.\n",
      "Comment 2400 of 11784.\n",
      "Comment 2500 of 11784.\n",
      "Comment 2600 of 11784.\n",
      "Comment 2700 of 11784.\n",
      "Comment 2800 of 11784.\n",
      "Comment 2900 of 11784.\n",
      "Comment 3000 of 11784.\n",
      "Comment 3100 of 11784.\n",
      "Comment 3200 of 11784.\n",
      "Comment 3300 of 11784.\n",
      "Comment 3400 of 11784.\n",
      "Comment 3500 of 11784.\n",
      "Comment 3600 of 11784.\n",
      "Comment 3700 of 11784.\n",
      "Comment 3800 of 11784.\n",
      "Comment 3900 of 11784.\n",
      "Comment 4000 of 11784.\n",
      "Comment 4100 of 11784.\n",
      "Comment 4200 of 11784.\n",
      "Comment 4300 of 11784.\n",
      "Comment 4400 of 11784.\n",
      "Comment 4500 of 11784.\n",
      "Comment 4600 of 11784.\n",
      "Comment 4700 of 11784.\n",
      "Comment 4800 of 11784.\n",
      "Comment 4900 of 11784.\n",
      "Comment 5000 of 11784.\n",
      "Comment 5100 of 11784.\n",
      "Comment 5200 of 11784.\n",
      "Comment 5300 of 11784.\n",
      "Comment 5400 of 11784.\n",
      "Comment 5500 of 11784.\n",
      "Comment 5600 of 11784.\n",
      "Comment 5700 of 11784.\n",
      "Comment 5800 of 11784.\n",
      "Comment 5900 of 11784.\n",
      "Comment 6000 of 11784.\n",
      "Comment 6100 of 11784.\n",
      "Comment 6200 of 11784.\n",
      "Comment 6300 of 11784.\n",
      "Comment 6400 of 11784.\n",
      "Comment 6500 of 11784.\n",
      "Comment 6600 of 11784.\n",
      "Comment 6700 of 11784.\n",
      "Comment 6800 of 11784.\n",
      "Comment 6900 of 11784.\n",
      "Comment 7000 of 11784.\n",
      "Comment 7100 of 11784.\n",
      "Comment 7200 of 11784.\n",
      "Comment 7300 of 11784.\n",
      "Comment 7400 of 11784.\n",
      "Comment 7500 of 11784.\n",
      "Comment 7600 of 11784.\n",
      "Comment 7700 of 11784.\n",
      "Comment 7800 of 11784.\n",
      "Comment 7900 of 11784.\n",
      "Comment 8000 of 11784.\n",
      "Comment 8100 of 11784.\n",
      "Comment 8200 of 11784.\n",
      "Comment 8300 of 11784.\n",
      "Comment 8400 of 11784.\n",
      "Comment 8500 of 11784.\n",
      "Comment 8600 of 11784.\n",
      "Comment 8700 of 11784.\n",
      "Comment 8800 of 11784.\n",
      "Comment 8900 of 11784.\n",
      "Comment 9000 of 11784.\n",
      "Comment 9100 of 11784.\n",
      "Comment 9200 of 11784.\n",
      "Comment 9300 of 11784.\n",
      "Comment 9400 of 11784.\n",
      "Comment 9500 of 11784.\n",
      "Comment 9600 of 11784.\n",
      "Comment 9700 of 11784.\n",
      "Comment 9800 of 11784.\n",
      "Comment 9900 of 11784.\n",
      "Comment 10000 of 11784.\n",
      "Comment 10100 of 11784.\n",
      "Comment 10200 of 11784.\n",
      "Comment 10300 of 11784.\n",
      "Comment 10400 of 11784.\n",
      "Comment 10500 of 11784.\n",
      "Comment 10600 of 11784.\n",
      "Comment 10700 of 11784.\n",
      "Comment 10800 of 11784.\n",
      "Comment 10900 of 11784.\n",
      "Comment 11000 of 11784.\n",
      "Comment 11100 of 11784.\n",
      "Comment 11200 of 11784.\n",
      "Comment 11300 of 11784.\n",
      "Comment 11400 of 11784.\n",
      "Comment 11500 of 11784.\n",
      "Comment 11600 of 11784.\n",
      "Comment 11700 of 11784.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "total_message = df.shape[0]\n",
    "clean_message = []\n",
    "\n",
    "print(\"Cleaning and parsing the message...\")\n",
    "\n",
    "j = 0\n",
    "for message in df['text']:\n",
    "    clean_message.append(message_to_words(message))\n",
    "    \n",
    "    # If the index is divisible by 100, print a message\n",
    "    if (j+1) % 100 == 0:\n",
    "        print(f'Comment {j+1} of {total_message}.')\n",
    "    \n",
    "    j += 1\n",
    "    \n",
    "    if j == total_message:\n",
    "        print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>username</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'block', 'of', 'ne', ...</td>\n",
       "      <td>med medical block ne th ave portland portland ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>mult co fire ems log</td>\n",
       "      <td>['med', 'medical', 'at', 'se', 'nd', 'ave', 's...</td>\n",
       "      <td>med medical se nd ave se johnson creek blvd po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>michellebot</td>\n",
       "      <td>['valley', 'of', 'fire', 'valley', 'of', 'fire...</td>\n",
       "      <td>valley fire valley fire state park</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1              username  \\\n",
       "0           0             0  mult co fire ems log   \n",
       "1           1             1  mult co fire ems log   \n",
       "2           2             2           michellebot   \n",
       "\n",
       "                                                text  \\\n",
       "0  ['med', 'medical', 'at', 'block', 'of', 'ne', ...   \n",
       "1  ['med', 'medical', 'at', 'se', 'nd', 'ave', 's...   \n",
       "2  ['valley', 'of', 'fire', 'valley', 'of', 'fire...   \n",
       "\n",
       "                                     cleaned_message  \n",
       "0  med medical block ne th ave portland portland ...  \n",
       "1  med medical se nd ave se johnson creek blvd po...  \n",
       "2                 valley fire valley fire state park  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.assign(cleaned_message = clean_message)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11784, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'username', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11784, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_list(dataframe):\n",
    "    \n",
    "    single_list = []\n",
    "    for row in dataframe['cleaned_message']:\n",
    "        for word in row:\n",
    "            single_list.append(word)\n",
    "            \n",
    "    return single_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_words = single_list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are some of the most common words contained in these tweets?\n",
    "# Method found on The Programming Historian: https://programminghistorian.org/en/lessons/counting-frequencies\n",
    "\n",
    "# Create an empty list\n",
    "word_frequency = []\n",
    "\n",
    "# Loop through the list of words and count each one up\n",
    "[word_frequency.append(text_words.count(word)) for word in text_words]\n",
    "\n",
    "#print(\"Pairs\\n\" + str(list(zip(text_words, word_frequency))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create blank dataframe\n",
    "# This dataframe is ONLY being used to count up the words that are found the most frequently in these tweets\n",
    "word_counts = pd.DataFrame()\n",
    "\n",
    "# Add desired columns from previously defined variables\n",
    "word_counts['words'] = text_words\n",
    "word_counts['word_frequency'] = word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts['words'].drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by word frequency\n",
    "word_counts.sort_values('word_frequency', ascending=False, inplace=True)\n",
    "\n",
    "# Drop duplicate words\n",
    "word_counts.drop_duplicates(subset = 'words', keep = 'first', inplace=True)\n",
    "\n",
    "# Look at the 20 most commonly used words\n",
    "word_counts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Urgent vs. non-Urgent Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpful medium post**\n",
    "https://medium.com/@belen.sanchez27/leveraging-social-media-to-map-disasters-74b4cc34848d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit to NYC group - their code provided a better visualization of the cosine similarity scores for each message\n",
    "#vector so when I started to have issues I found their code and applied it while I am tweeting my bags of words\n",
    "#for a clearer understanding of the cosine similarity scores \n",
    "\n",
    "\n",
    "#Function for vectorization of corpus \n",
    "def tweet_vector(bag_of_words_list):\n",
    "    \n",
    "    #Counter for number of words in corpus that exists in GoogleNews word list \n",
    "    counter=0\n",
    "   \n",
    "    #Creating a template for cumulative corpus vector sum (300 dimensions)\n",
    "    tweet_vector_sum = np.zeros((1,300))\n",
    "    \n",
    "    #Iterating over each word in bag of word list \n",
    "    for word in bag_of_words_list:\n",
    "\n",
    "        #Checking if word exists in GoogleNews word list\n",
    "        if word in model.vocab:                    \n",
    "            \n",
    "            #Vectorizing the word if in list\n",
    "            word_vec = model.word_vec(word)        \n",
    "            \n",
    "            #Updating counter\n",
    "            counter += 1\n",
    "            \n",
    "            #Updating cumulative vector sum \n",
    "            tweet_vector_sum = tweet_vector_sum + word_vec \n",
    "\n",
    "    #Computing average vector by taking cumulative vector sum and dividing it by number of words traced\n",
    "    tweet_vector_avg = tweet_vector_sum/counter\n",
    "    \n",
    "    #Using numpy to squeeze N-dimensional nested array object into a 1-D array \n",
    "    tweet_vector_avg = np.squeeze(tweet_vector_avg)\n",
    "    \n",
    "    return(tweet_vector_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying function to vectorize both bag of words list\n",
    "urgent_vec = tweet_vector(urgent)\n",
    "nonurgent_vec = tweet_vector(non_urgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urgent_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonurgent_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function for computing cosine similarity score \n",
    "def cos_sim_score(a,b): \n",
    "    \n",
    "    #Calculating the dot product of two vectors\n",
    "    dot = np.dot(a, b)\n",
    "    \n",
    "    #Calculating the magnitude of the vector \n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    \n",
    "    #Calculating cosine similarity\n",
    "    cos = dot / (norma * normb)\n",
    "    \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urgent_vec.reshape(1, -1)\n",
    "#nonurgent_vec.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #working copy \n",
    "# #Empty lists\n",
    "# urgent_cos_sim = []\n",
    "# non_urgent_cos_sim = []\n",
    "\n",
    "# #Iterating through each tweet in out list of tokens\n",
    "# for tweet in df['cleaned_message']:\n",
    "    \n",
    "#     #Call function \n",
    "#     avg_vec_per_tweet = tweet_vector(tweet)\n",
    "    \n",
    "#     urgent_vec = urgent_vec.reshape(1, -1)\n",
    "#     nonurgent_vec = nonurgent_vec.reshape(1, -1)\n",
    "#     avg_vec_per_tweet = avg_vec_per_tweet.reshape(1, -1)\n",
    "    \n",
    "#     #Creating new column in df for cosine simarlity score \n",
    "#     urgent_cos_sim.append(cosine_similarity(avg_vec_per_tweet, urgent_vec))\n",
    "#     urgent_cos_sim.append(cosine_similarity(avg_vec_per_tweet, nonurgent_vec))\n",
    "#     #urgent_cos_sim.append(cos_sim_score(avg_vec_per_tweet, urgent_vec))\n",
    "#     #non_urgent_cos_sim.append(cos_sim_score(avg_vec_per_tweet, nonurgent_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original copy - DO NOT CHANGE\n",
    "#Empty lists\n",
    "urgent_cos_sim = []\n",
    "non_urgent_cos_sim = []\n",
    "\n",
    "#Iterating through each tweet in out list of tokens\n",
    "for tweet in df['cleaned_message']:\n",
    "    \n",
    "    #Call function \n",
    "    avg_vec_per_tweet = tweet_vector(tweet)\n",
    "    \n",
    "    #Creating new column in df for cosine simarlity score \n",
    "    urgent_cos_sim.append(cos_sim_score(avg_vec_per_tweet, urgent_vec))\n",
    "    non_urgent_cos_sim.append(cos_sim_score(avg_vec_per_tweet, nonurgent_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>urgent_cos_sim</th>\n",
       "      <th>non_urgent_cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>med medical block ne th ave portland portland ...</td>\n",
       "      <td>0.148701</td>\n",
       "      <td>0.137452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>med medical se nd ave se johnson creek blvd po...</td>\n",
       "      <td>0.149998</td>\n",
       "      <td>0.135317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>valley fire valley fire state park</td>\n",
       "      <td>0.144539</td>\n",
       "      <td>0.141861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>med medical block ne rodney ave portland portl...</td>\n",
       "      <td>0.145074</td>\n",
       "      <td>0.131240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>special shout llyk comin fire video downtown l...</td>\n",
       "      <td>0.144783</td>\n",
       "      <td>0.133166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     cleaned_message  urgent_cos_sim  \\\n",
       "0  med medical block ne th ave portland portland ...        0.148701   \n",
       "1  med medical se nd ave se johnson creek blvd po...        0.149998   \n",
       "2                 valley fire valley fire state park        0.144539   \n",
       "3  med medical block ne rodney ave portland portl...        0.145074   \n",
       "4  special shout llyk comin fire video downtown l...        0.144783   \n",
       "\n",
       "   non_urgent_cos_sim  \n",
       "0            0.137452  \n",
       "1            0.135317  \n",
       "2            0.141861  \n",
       "3            0.131240  \n",
       "4            0.133166  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['urgent_cos_sim'] = urgent_cos_sim\n",
    "df['non_urgent_cos_sim'] = non_urgent_cos_sim\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Classification \n",
    "urgent = []\n",
    "\n",
    "for rows in df.index:  \n",
    "    if df['urgent_cos_sim'][rows] > df['non_urgent_cos_sim'][rows]:\n",
    "        urgent.append(1)\n",
    "    else: \n",
    "        urgent.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a new column\n",
    "df['urgent'] = urgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.64927\n",
       "0    0.35073\n",
       "Name: urgent, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['urgent'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>urgent_cos_sim</th>\n",
       "      <th>non_urgent_cos_sim</th>\n",
       "      <th>urgent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smokea smoke investigation outside structure n...</td>\n",
       "      <td>0.158059</td>\n",
       "      <td>0.164721</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>med medical se nd ave se morrison st morrison ...</td>\n",
       "      <td>0.141777</td>\n",
       "      <td>0.144964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>smokea smoke investigation outside structure s...</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.155813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>buy two get one free decorate tree ghoulish de...</td>\n",
       "      <td>0.142310</td>\n",
       "      <td>0.147313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bay area thankful rain clean air fire brutal n...</td>\n",
       "      <td>0.142018</td>\n",
       "      <td>0.143126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>smokea smoke investigation outside structure n...</td>\n",
       "      <td>0.158059</td>\n",
       "      <td>0.164721</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>med medical se nd ave se morrison st morrison ...</td>\n",
       "      <td>0.141777</td>\n",
       "      <td>0.144964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>smokea smoke investigation outside structure s...</td>\n",
       "      <td>0.149254</td>\n",
       "      <td>0.155813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>buy two get one free decorate tree ghoulish de...</td>\n",
       "      <td>0.142310</td>\n",
       "      <td>0.147313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>bay area thankful rain clean air fire brutal n...</td>\n",
       "      <td>0.142018</td>\n",
       "      <td>0.143126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>want smoke linkinbio zeroaliasmusic thecoldest...</td>\n",
       "      <td>0.139762</td>\n",
       "      <td>0.140521</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>today grateful relief fire smoke shot one clea...</td>\n",
       "      <td>0.133591</td>\n",
       "      <td>0.134483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>smoke smoke investigation inside structure blo...</td>\n",
       "      <td>0.154809</td>\n",
       "      <td>0.154943</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>rfire residential structure fire block se th a...</td>\n",
       "      <td>0.144933</td>\n",
       "      <td>0.148972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>smokea smoke investigation outside structure s...</td>\n",
       "      <td>0.153651</td>\n",
       "      <td>0.159442</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>grateful support calfire pio team chief pio of...</td>\n",
       "      <td>0.149404</td>\n",
       "      <td>0.150706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>mineralfire unofficial new fire report detail ...</td>\n",
       "      <td>0.140783</td>\n",
       "      <td>0.144771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>pet paradise syntheticgrass green savingwater ...</td>\n",
       "      <td>0.140078</td>\n",
       "      <td>0.141398</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>rain hinders search help nearly put camp fire</td>\n",
       "      <td>0.139970</td>\n",
       "      <td>0.140032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>amazing creature standing last year quite fire...</td>\n",
       "      <td>0.140624</td>\n",
       "      <td>0.145591</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cleaned_message  urgent_cos_sim  \\\n",
       "5   smokea smoke investigation outside structure n...        0.158059   \n",
       "9   med medical se nd ave se morrison st morrison ...        0.141777   \n",
       "10  smokea smoke investigation outside structure s...        0.149254   \n",
       "11  buy two get one free decorate tree ghoulish de...        0.142310   \n",
       "12  bay area thankful rain clean air fire brutal n...        0.142018   \n",
       "25  smokea smoke investigation outside structure n...        0.158059   \n",
       "29  med medical se nd ave se morrison st morrison ...        0.141777   \n",
       "30  smokea smoke investigation outside structure s...        0.149254   \n",
       "31  buy two get one free decorate tree ghoulish de...        0.142310   \n",
       "32  bay area thankful rain clean air fire brutal n...        0.142018   \n",
       "44  want smoke linkinbio zeroaliasmusic thecoldest...        0.139762   \n",
       "49  today grateful relief fire smoke shot one clea...        0.133591   \n",
       "52  smoke smoke investigation inside structure blo...        0.154809   \n",
       "53  rfire residential structure fire block se th a...        0.144933   \n",
       "61  smokea smoke investigation outside structure s...        0.153651   \n",
       "65  grateful support calfire pio team chief pio of...        0.149404   \n",
       "66  mineralfire unofficial new fire report detail ...        0.140783   \n",
       "70  pet paradise syntheticgrass green savingwater ...        0.140078   \n",
       "73      rain hinders search help nearly put camp fire        0.139970   \n",
       "78  amazing creature standing last year quite fire...        0.140624   \n",
       "\n",
       "    non_urgent_cos_sim  urgent  \n",
       "5             0.164721       0  \n",
       "9             0.144964       0  \n",
       "10            0.155813       0  \n",
       "11            0.147313       0  \n",
       "12            0.143126       0  \n",
       "25            0.164721       0  \n",
       "29            0.144964       0  \n",
       "30            0.155813       0  \n",
       "31            0.147313       0  \n",
       "32            0.143126       0  \n",
       "44            0.140521       0  \n",
       "49            0.134483       0  \n",
       "52            0.154943       0  \n",
       "53            0.148972       0  \n",
       "61            0.159442       0  \n",
       "65            0.150706       0  \n",
       "66            0.144771       0  \n",
       "70            0.141398       0  \n",
       "73            0.140032       0  \n",
       "78            0.145591       0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['urgent'] == 0].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ta p traffic accident pin block ne th ave portland portland fire rp pdx'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#urgent\n",
    "df['cleaned_message'][35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'med medical block ne rodney ave portland portland fire rp pdx'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#urgent\n",
    "df['cleaned_message'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'smokea smoke investigation outside structure n vancouver ave columbia slough multnomah county portland fire rp'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_message'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"../datasets/raw_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    MED - MEDICAL at 700 BLOCK OF NE 78TH AVE, POR...\n",
       "1    MED - MEDICAL at SE 32ND AVE / SE JOHNSON CREE...\n",
       "2    Valley of Fire @ Valley of Fire State Park htt...\n",
       "3    MED - MEDICAL at 2000 BLOCK OF NE RODNEY AVE, ...\n",
       "4    Special shout out to @m0llyk4y for comin throu...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    med medical block ne th ave portland portland ...\n",
       "1    med medical se nd ave se johnson creek blvd po...\n",
       "2                   valley fire valley fire state park\n",
       "3    med medical block ne rodney ave portland portl...\n",
       "4    special shout llyk comin fire video downtown l...\n",
       "Name: cleaned_message, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_message'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df_original[['text','username']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MED - MEDICAL at 700 BLOCK OF NE 78TH AVE, POR...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MED - MEDICAL at SE 32ND AVE / SE JOHNSON CREE...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Valley of Fire @ Valley of Fire State Park htt...</td>\n",
       "      <td>MichelleBot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MED - MEDICAL at 2000 BLOCK OF NE RODNEY AVE, ...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Special shout out to @m0llyk4y for comin throu...</td>\n",
       "      <td>Houdini</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text              username\n",
       "0  MED - MEDICAL at 700 BLOCK OF NE 78TH AVE, POR...  Mult Co Fire/EMS log\n",
       "1  MED - MEDICAL at SE 32ND AVE / SE JOHNSON CREE...  Mult Co Fire/EMS log\n",
       "2  Valley of Fire @ Valley of Fire State Park htt...           MichelleBot\n",
       "3  MED - MEDICAL at 2000 BLOCK OF NE RODNEY AVE, ...  Mult Co Fire/EMS log\n",
       "4  Special shout out to @m0llyk4y for comin throu...               Houdini"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_original, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>urgent_cos_sim</th>\n",
       "      <th>non_urgent_cos_sim</th>\n",
       "      <th>urgent</th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>black friday release sublime greatest hit firs...</td>\n",
       "      <td>0.138924</td>\n",
       "      <td>0.137414</td>\n",
       "      <td>1</td>\n",
       "      <td>with the Black Friday releases of Sublime-Gre...</td>\n",
       "      <td>Programme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5749</th>\n",
       "      <td>med medical block ne sandy blvd portland portl...</td>\n",
       "      <td>0.145306</td>\n",
       "      <td>0.128034</td>\n",
       "      <td>1</td>\n",
       "      <td>MED - MEDICAL at 8300 BLOCK OF NE SANDY BLVD, ...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>almcom monitored commercial fire alarm block s...</td>\n",
       "      <td>0.142837</td>\n",
       "      <td>0.145155</td>\n",
       "      <td>0</td>\n",
       "      <td>ALMCOM - MONITORED COMMERCIAL FIRE ALARM at 14...</td>\n",
       "      <td>Mult Co Fire/EMS log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4594</th>\n",
       "      <td>much latergram actually posting copy angievelv...</td>\n",
       "      <td>0.144122</td>\n",
       "      <td>0.143888</td>\n",
       "      <td>1</td>\n",
       "      <td>Much, #latergram I’m actually posting to copy ...</td>\n",
       "      <td>Felicity Kay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10888</th>\n",
       "      <td>smoke campfire thick see bay barely see emeryv...</td>\n",
       "      <td>0.138019</td>\n",
       "      <td>0.139797</td>\n",
       "      <td>0</td>\n",
       "      <td>The smoke from the #CampFire is so thick you c...</td>\n",
       "      <td>Safer at home, Yoda is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         cleaned_message  urgent_cos_sim  \\\n",
       "1353   black friday release sublime greatest hit firs...        0.138924   \n",
       "5749   med medical block ne sandy blvd portland portl...        0.145306   \n",
       "4008   almcom monitored commercial fire alarm block s...        0.142837   \n",
       "4594   much latergram actually posting copy angievelv...        0.144122   \n",
       "10888  smoke campfire thick see bay barely see emeryv...        0.138019   \n",
       "\n",
       "       non_urgent_cos_sim  urgent  \\\n",
       "1353             0.137414       1   \n",
       "5749             0.128034       1   \n",
       "4008             0.145155       0   \n",
       "4594             0.143888       1   \n",
       "10888            0.139797       0   \n",
       "\n",
       "                                                    text  \\\n",
       "1353    with the Black Friday releases of Sublime-Gre...   \n",
       "5749   MED - MEDICAL at 8300 BLOCK OF NE SANDY BLVD, ...   \n",
       "4008   ALMCOM - MONITORED COMMERCIAL FIRE ALARM at 14...   \n",
       "4594   Much, #latergram I’m actually posting to copy ...   \n",
       "10888  The smoke from the #CampFire is so thick you c...   \n",
       "\n",
       "                     username  \n",
       "1353                Programme  \n",
       "5749     Mult Co Fire/EMS log  \n",
       "4008     Mult Co Fire/EMS log  \n",
       "4594             Felicity Kay  \n",
       "10888  Safer at home, Yoda is  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TA1 - TRAFFIC ACCIDENT - 1ST RESPONSE (FIRE & EMS) at EB I84 FWY WO / EXIT 5 & NE 82ND AVE, PORTLAND, OR [Portland Fire #RP18000092329]'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ugent example\n",
    "df['text'][1785]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My morning  weather report. Smoke? Wind has blown smoke from the horrible fires  in Malibu, Thousand Oaks, Agoura Hills all over Los Angeles. Pray everyone… https://www.instagram.com/p/BqDMM2eBV5EhDie9Z0s5ycTcXZPq9bOGzQFZMs0/?utm_source=ig_twitter_share&igshid=kmuy4xajhwju\\xa0…'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non-urgent example\n",
    "df['text'][9746]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My birthday gift redeemed, a visit with the Giant Sequoia survivors of Fire.  #tbt #DaughteroftheSouth #forests #conservation #sentientbeings @ Sequoia & Kings Canyon National Parks -… https://www.instagram.com/p/BqODiFSlZRQ/?utm_source=ig_twitter_share&igshid=1fa67qnjkn6m5\\xa0…'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non-urgent example\n",
    "df['text'][4633]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two right lanes blocked-big rig was on fire on 805 SB at Palm Ave #SDtraffic http://bit.ly/Y31oyM\\xa0'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][5607]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'APPLI - APPLIANCE OR EQUIPMENT FIRE at 400 BLOCK OF SE 127TH AVE, PORTLAND, OR [Portland Fire #RP18000094181] 10:35 #pdx911'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][6182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GRASS - GRASS, BARKDUST OR TREE FIRE at 5000 BLOCK OF NE 6TH AVE, PORTLAND, OR [Portland Fire #RP18000095796] 04:39 #pdx911'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][2165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An eerily hazy view down Market St at 2pm this Friday from the smoke and poor air quality due to the Wildfires burning 150+ miles north of the Bay Area.\\n•\\n#Smoke #Haze #CampFire #Wildfire… https://www.instagram.com/p/BqRPwWvBWLf/?utm_source=ig_twitter_share&igshid=1vwr2ycd8f9ke\\xa0…'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not urgent\n",
    "df['text'][4412]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carr Fire, Camp Fire Smoke, Soot, Ash, Odor and Fire Damage Clean Up\\nhttps://bit.ly/2K344fA\\xa0\\n#carrfire #campfire #shastacounty #servpro #reddingcaliforniapic.twitter.com/NHyssLjSnR'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#urgent\n",
    "df['text'][1084]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very eerie day for Channel Islands Beach as the #smoke blankets the Sun. Praying for those effected.  by the #hillfire @visitventura #wildfire #fire visitoxnardca #ominous #emptybeach… https://www.instagram.com/p/Bp-GPvuhaV_/?utm_source=ig_twitter_share&igshid=1kjuq70ahft77\\xa0…'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#urgent\n",
    "df['text'][8693]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Nov 9th, 2018, CO dominant, firestorm smoke with probable contamination”.  In preparation for future extreme weather events we need to prioritize the containment and cleanup of toxic… https://www.instagram.com/p/BqGcoJbH1Yl/?utm_source=ig_twitter_share&igshid=1bydhyyy7aj4d\\xa0…'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#urgent\n",
    "df['text'][7470]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vehicle on fire on I-80 WB near Echo Cyn Rd #SLCtraffic http://bit.ly/Xid4QI\\xa0'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#urgent\n",
    "df['text'][3611]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!! sigalert !! the road is closed because of a brush fire. in #Grapevine on I-5 NB at Smokey Bear Rd, stopped traffic back to Templin Hwy'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][6104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fire next to home. Looks like someone started on purpose! @ Sorrento, California https://www.instagram.com/p/BqLuqRXHoJN/?utm_source=ig_twitter_share&igshid=sykx448gicjo\\xa0…'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][5193]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Camp fire closure in #Oroville on Hwy 70 Both NB/SB between Pentz Rd and CA 89 #traffic http://bit.ly/Y31VAF\\xa0'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Urgent\n",
    "df['text'][2586]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Camp fire closure in #Oroville on 191 Both NB/SB between Durham-Pentz Rd and before Pearson Rd #traffic http://bit.ly/Y31VAF\\xa0'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Urgent\n",
    "df['text'][8195]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Searches Intensify With More Than 600 Reported Missing in Butte County’s Camp\\xa0Fire http://bit.ly/2K8GLkp\\xa0'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NonUrgent\n",
    "df['text'][5565]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Really bad. So bad I called it in to the NWS (I’m a trained skywarn spotter) and they issued a Dense Smoke Advisory. @ Roseville, California https://www.instagram.com/p/BqBMDGjl2Njf9eO_zgiTRXZ7vkg61XF0PUxgeI0/?utm_source=ig_twitter_share&igshid=3vujyep2jbr1\\xa0…'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NonUrgent\n",
    "df['text'][10587]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Camp Fire smoke making it look like Mars rising in the East.\\n\\nReally, it looked much bigger than that. @ Sausalito, California https://www.instagram.com/p/Bp9tqe5gU4-/?utm_source=ig_twitter_share&igshid=18sy347f6epv6\\xa0…'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NonUrgent\n",
    "df['text'][9012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Same chill in my blood as the ‘94 Malibu fires which were only stopped by the Pacific Ocean itself.  We live in chaparral and choose to live amongst nature’s will but it’s still… https://www.instagram.com/p/Bp_GMEhlKA2/?utm_source=ig_twitter_share&igshid=u3yj0qprdr5l\\xa0…'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NonUrgent\n",
    "df['text'][11517]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You got snow and we got smoke.'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NonUrgent\n",
    "df['text'][5712]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example = df.iloc[[5193],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exampleb = df.iloc[[9012],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example = df_exampleb.append(df_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_message</th>\n",
       "      <th>urgent_cos_sim</th>\n",
       "      <th>non_urgent_cos_sim</th>\n",
       "      <th>urgent</th>\n",
       "      <th>text</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9012</th>\n",
       "      <td>camp fire smoke making look like mar rising ea...</td>\n",
       "      <td>0.141834</td>\n",
       "      <td>0.147395</td>\n",
       "      <td>0</td>\n",
       "      <td>Camp Fire smoke making it look like Mars risin...</td>\n",
       "      <td>Frank Leahy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>fire next home look like someone started purpo...</td>\n",
       "      <td>0.145962</td>\n",
       "      <td>0.153339</td>\n",
       "      <td>0</td>\n",
       "      <td>Fire next to home. Looks like someone started ...</td>\n",
       "      <td>1John 🇺🇸🇨🇮</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_message  urgent_cos_sim  \\\n",
       "9012  camp fire smoke making look like mar rising ea...        0.141834   \n",
       "5193  fire next home look like someone started purpo...        0.145962   \n",
       "\n",
       "      non_urgent_cos_sim  urgent  \\\n",
       "9012            0.147395       0   \n",
       "5193            0.153339       0   \n",
       "\n",
       "                                                   text     username  \n",
       "9012  Camp Fire smoke making it look like Mars risin...  Frank Leahy  \n",
       "5193  Fire next to home. Looks like someone started ...   1John 🇺🇸🇨🇮  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_example.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example = df_example.rename(columns={\"text\": \"tweet\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example = df_example[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_example.to_csv('./sample_points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urgent = df.loc[df['urgent'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonurgent = df.loc[df['urgent'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_urgent.to_csv('..//datasets/df_urgent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonurgent.to_csv('..//datasets/df_nonurgent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urgent_vector = np.zeros((1,300))\n",
    "# count = 0\n",
    "# for word in urgent:\n",
    "#     if word not in model.vocab:\n",
    "#         continue\n",
    "#     else:\n",
    "#         temp = model.word_vec(word)\n",
    "#         emerg_vect = urgent_vector + temp\n",
    "#         count +=1\n",
    "\n",
    "# urgent_vector = urgent_vector/count\n",
    "# urgent_vector = np.squeeze(urgent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_urgent_vector = np.zeros((1,300))\n",
    "# count = 0\n",
    "# for word in non_urgent:\n",
    "    \n",
    "#     if word not in model.vocab:\n",
    "#         continue\n",
    "#     else:\n",
    "#         temp = model.word_vec(word)\n",
    "#         permanent_vect = non_urgent_vector + temp\n",
    "#         count +=1\n",
    "\n",
    "# non_urgent_vector = non_urgent_vector/count\n",
    "# non_urgent_vector = np.squeeze(non_urgent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-66c791a440fe>:26: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if np.dot(message_vector, urgent_vector)/(np.linalg.norm(urgent_vector)*np.linalg.norm(message_vector)) >= np.dot(message_vector,non_urgent_vector)/(np.linalg.norm(non_urgent_vector)*np.linalg.norm(message_vector)):\n"
     ]
    }
   ],
   "source": [
    "# #Where our classifications will go\n",
    "# urgent_message = [] \n",
    "# #for a tweet in our clean tweet tokes\n",
    "# for tweet in df['text']:\n",
    "# #set up counter    \n",
    "#     count = 0\n",
    "#     #for each token in tweet\n",
    "#     for token in tweet:\n",
    "#         #set up a message vector that is size of 300\n",
    "#         message_vector = np.zeros((1, 300))\n",
    "#         # if token is not in Word2Vec model, do not include\n",
    "#         if token not in model.vocab.keys(): \n",
    "#             continue\n",
    "#         else:\n",
    "#             message_vector = message_vector + model.word_vec(token)\n",
    "#             count += 1\n",
    "#     if count == 0:\n",
    "#         count = 1\n",
    "#     message_vector = np.squeeze(message_vector)/count\n",
    "    \n",
    "    \n",
    "#     #Calculate the dot product for each vector (urgent or non-urgent), then through cosine similarity assign\n",
    "#     #whether the message is urgent or not urgent based on if the message is more similar to urgent or non-urgent\n",
    "#     #score\n",
    "    \n",
    "#     if np.dot(message_vector, urgent_vector)/(np.linalg.norm(urgent_vector)*np.linalg.norm(message_vector)) >= np.dot(message_vector,non_urgent_vector)/(np.linalg.norm(non_urgent_vector)*np.linalg.norm(message_vector)):\n",
    "#         urgent_message.append(1)\n",
    "#     else:\n",
    "#         urgent_message.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add classification to df \n",
    "# df['urgent_message'] = urgent_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    11784\n",
       "Name: urgent_message, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['urgent_message'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
