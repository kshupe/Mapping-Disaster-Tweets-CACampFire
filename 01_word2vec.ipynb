{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Bag-of-Words\" data-toc-modified-id=\"Bag-of-Words-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Bag of Words</a></span></li><li><span><a href=\"#Train-Words2Vec\" data-toc-modified-id=\"Train-Words2Vec-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Train Words2Vec</a></span></li><li><span><a href=\"#Creating-the-Urgent-vs.-non-Urgent-Vectors\" data-toc-modified-id=\"Creating-the-Urgent-vs.-non-Urgent-Vectors-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Creating the Urgent vs. non-Urgent Vectors</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **words were created using a logistic regression on an open source database used for NLP that contains messages during disasters and their categorizations, one of those being if the message was a direct call for help, what we will classify as a message being more \"urgent\"**\n",
    "    - https://appen.com/datasets/combined-disaster-response-data/\n",
    "    \n",
    "    \n",
    "- **another way to classify is a message is more urgent is using FEMA's Public Assistance Program and Policy Guide**\n",
    "    - in this guide, public assistance work is either emergency work or permanent work. Important terms from each designation were added to our bag of words \n",
    "    - **Emergency**: \n",
    "        - Emergency Protective Measures\n",
    "        - Debris Removal \n",
    "    - **Permanent**:\n",
    "        - Roads and Bridges\n",
    "        - Water Control Facilities\n",
    "        - Buildings and Equipment\n",
    "        - Utilities \n",
    "        - Parks, Recreational, other \n",
    "        \n",
    "    - https://www.fema.gov/media-library-data/1525468328389-4a038bbef9081cd7dfe7538e7751aa9c/PAPPG_3.1_508_FINAL_5-4-2018.pdf\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of urgent words\n",
    "urgent = ['help',\n",
    "         'fire',\n",
    "          'need',\n",
    "          'hungry',\n",
    "          'aid',\n",
    "          'removal',\n",
    "          'tent',\n",
    "          'dying',\n",
    "          'evacuation',\n",
    "          'starving',\n",
    "          'smoke',\n",
    "          'shelter',\n",
    "          'debris',\n",
    "          'unsafe',\n",
    "          'access',\n",
    "          'rescue',\n",
    "          'search',\n",
    "          'lost',\n",
    "          'victim',\n",
    "          'sos'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of non-urgent words \n",
    "non_urgent = ['job',\n",
    "              'news',\n",
    "              'government',\n",
    "              'country',\n",
    "              'reported',\n",
    "              'materials',\n",
    "              'work',\n",
    "              'price',\n",
    "              'utilities',\n",
    "              'facility'\n",
    "              'parks',\n",
    "              'playground',\n",
    "              'bridge',\n",
    "              'sidewalk',\n",
    "              'guardrails',\n",
    "              'erosion',\n",
    "              'irrigation',\n",
    "              'vegetation',\n",
    "              'traffic',\n",
    "              'restoration',\n",
    "              'inspection',\n",
    "              'assessment'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Words2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('..//data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking vector size of GoogleNews Word2Vec\n",
    "model.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Urgent vs. non-Urgent Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpful medium post**\n",
    "https://medium.com/@belen.sanchez27/leveraging-social-media-to-map-disasters-74b4cc34848d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urgent_vector = np.zeros((1,300))\n",
    "count = 0\n",
    "for word in urgent:\n",
    "    if word not in model.vocab:\n",
    "        continue\n",
    "    else:\n",
    "        temp = model.word_vec(word)\n",
    "        emerg_vect = urgent_vector + temp\n",
    "        counter +=1\n",
    "\n",
    "urgent_vector = urgent_vector/counter\n",
    "urgent_vector = np.squeeze(urgent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_urgent_vector = np.zeros((1,300))\n",
    "count = 0\n",
    "for word in non_urgent:\n",
    "    \n",
    "    if word not in model.vocab:\n",
    "        continue\n",
    "    else:\n",
    "        temp = model.word_vec(word)\n",
    "        permanent_vect = non_urgent_vector + temp\n",
    "        counter +=1\n",
    "\n",
    "non_urgent_vector = non_urgent_vector/counter\n",
    "non_urgent_vector = np.squeeze(non_urgent_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where our classifications will go\n",
    "urgent_message = [] \n",
    "#for a tweet in our clean tweet tokes\n",
    "for tweet in clean_tweets:\n",
    "#set up counter    \n",
    "    count = 0\n",
    "    #for each token in tweet\n",
    "    for token in tweet:\n",
    "        #set up a message vector that is size of 300\n",
    "        message_vector = np.zeros((1, 300))\n",
    "        # if token is not in Word2Vec model, do not include\n",
    "        if token not in model.vocab.keys(): \n",
    "            continue\n",
    "        else:\n",
    "            message_vector = message_vector + model.word_vec(token)\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    message_vector = np.squeeze(message_vector)/count\n",
    "    \n",
    "    \n",
    "    #Calculate the dot product for each vector (urgent or non-urgent), then through cosine similarity assign\n",
    "    #whether the message is urgent or not urgent based on if the message is more similar to urgent or non-urgent\n",
    "    #score\n",
    "    \n",
    "    if np.dot(message_vector, urgent_vector)/(np.linalg.norm(urgent_vector)*np.linalg.norm(message_vector)) >= np.dot(message_vector, non_urgent_vector)/(np.linalg.norm(non_urgent_vector)*np.linalg.norm(message_vector)):\n",
    "        urgent_message.append(1)\n",
    "    else:\n",
    "        urgent_message.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add classification to df \n",
    "df['urgent_message'] = urgent_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
